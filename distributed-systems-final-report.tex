\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{float}
\usepackage{cleveref} % this must be the last package to be loaded

\newcommand{\emailaddr}[1]{\href{mailto:#1}{\texttt{#1}}}

\title{\LARGE
    Final Report\\
    OPC UA Industrial Plant HUB
}
\subtitle{Final Report for the Distributed Systems Course [A.A 2025-2026]}

% Consider watching:
% https://www.youtube.com/watch?v=ihxSUsJB_14
% https://www.youtube.com/watch?v=XTFWaV55uDo

\author{
    Federico Capitanio\\ \emailaddr{federico.capitanio@studio.unibo.it}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    This report illustrates \textbf{OPCUA Industrial Plant Hub}, a software developed
    to demonstrate the fundamental principles of Distributed Systems by integrating them with the world
    of industrial automation.

    In the world of industrial automation, the issue of data collection from machines and machine lines is becoming increasingly important. Therefore, this project aims to provide a distributed data collection system for different production lines, which may also be distributed across the territory.

    The project is based on the use of IoT devices, which are connected to the production line and collect data on the production process. The data collected is then sent to a central server, which stores it and allows it to be analysed. The data can also be sent to the machines themselves, allowing them to be controlled and optimised.

    The underlying idea is that data is collected from the machine line using the \textbf{OPC UA} protocol, 
    widely used in industry, saved and then
    retrieved via \textbf{HTTP API}.

    Unlike traditional data collection architectures based on a single
    physical node that reads data, the architecture of this project consists of three
    HUB nodes that collect \textbf{OPC UA} data, synchronised via Anti-Entropy and Gossip protocols,
    with an nginx load balancer that distributes client requests and implements
    data encryption via the \textbf{TLS} protocol.

    The project emphasises the \textbf{AP} behaviour of the \textbf{CAP Theorem}, i.e. it focuses
    on the aspects of \textbf{Partition Tolerance} and \textbf{High Availability}, thus ensuring
    that clients can always access data even in the presence of network partitions
    or individual node failures.
    The \textbf{Strong Consistency} concept is therefore not guaranteed, promoting
    data that is not updated to the latest possible value over a longer wait time that would result
    from the application of \textbf{Strong Consistency} itself.
    
    The system allows OPC UA sources to be added dynamically, by implementing
    an \textbf{on the fly} automation line connection mode, allowing 
    individual machines or entire lines to be connected and added without the system
    needing to be restarted.


\end{abstract}

\newpage


\section{Concept}\label{concept}

This project focuses on the development of a distributed system for collecting industrial data from different lines of automatic machines or plants, allowing for its centralisation.

\subsection{Product Type}\label{product-type}

\begin{itemize}
  \item A web interface that shows all the possible HTTP APIs and make HTTP requests available directly from the page itself;
  \item Setup scripts, one for Windows and one for Linux, to create user credentials that will allow ;
  \item OPC UA server simulator to instance various OPC UA nodes and each one is able simulate real machine data;
  \item CLI commands to test the system itself, for instance switching off a node and restart it later.

\end{itemize}

\subsection{Use Case Description}\label{use-case-description}

\begin{itemize}
  \item \emph{What is the software doing?}
  The software initially instantiates three different nodes (there may be more). Each node connects to several OPC UA servers, browses all the variables present within the server itself, connects and starts collecting values every N seconds.
  The collected data is saved locally and replicated via Anti-Entropy with the other nodes performing the same task.
  The nodes also exchange messages with each other to know which nodes are actually active.
  An nginx server acting as a load balancer redirects HTTP requests coming from outside to the nodes that are alive and periodically tries to determine which of the previously down nodes are back online.
  
  \item \emph{when and how frequently do they interact with the system?}
  Users who will use the application will be anyone who makes an HTTP request to port 443 of the nginx server.
  
  \item \emph{when and how frequently do they interact with the system?}
  Since the software is not something like a game, users are defined as those who use the system itself and they can sent requests at any time as long as they don't exceed the Nginx brute force attack protection.
  
  \item \emph{how do they interact with the system? which devices are they using?}
  The user could interact with the system with every HTTP(s) client, so GUI client like Postman, CLI commands like \emph{curl} or even
  by building a front-end web page that use these application's API to show interactive web page to monitor machine performance or send command to the application itself.
  
\end{itemize}


\subsection{Why is distribution needed?}\label{why-distribution}

Distribution is a strictly necessary choice, as availability and reliability requirements are non-negotiable and constantly demanded in
the industrial sector. Below are the main reasons why distribution is necessary.

\begin{itemize}
  \item \emph{Fault Tolerance:}
  Each node (Ingestor) operates autonomously, collecting data from various OPC UA servers.
  Therefore, if any node fails, the system continues to operate with the remaining nodes.
  Furthermore, when this happens, Anti-Entropy mechanisms ensure automatic resynchronisation once
  the node (or nodes) becomes operational again.
  \item \emph{High Availability:}
  Industrial plants are designed to operate 24/7 most of the time, requiring
  constant monitoring systems with high availability.
  \item \emph{Scalability:}
  The possible increase in the number of monitored machines or lines themselves and the possibility that more
  users and applications will need access to the data.
  \item \emph{Geographic Distribution:}
  Various industrial companies operate on sites that can cover different geographically distributed areas,
  with different plants in different national and international areas.
  Potentially, through the evolution of the system, clients will be able to connect to the node
  that is physically closest to them.

\end{itemize}


\section{Requirements Elicitation and Analysis}\label{requirements}
This section aims to outline the system requirements and their implementation
to explain what the software must actually do. The main focus is to explain what
the supplied software must do when it comes into operation.


\subsection{Glossary}\label{glossary}

Before proceeding with the requirements analysis, it is necessary to define some
domain-specific terms:

\begin{description}
    \item[OPC UA - Open Platform Communications Unified Architecture]~\cite{da2023survey}
    An open international standard for secure and reliable data exchange, essential in the world of
    Industry 4.0 and industrial IoT. It was developed by the OPC Foundation and enables interoperability
    between heterogeneous devices and systems (PLCs, HMIs, clouds) regardless of manufacturer or
    operating system.
    Its strengths are the ability to leverage the integrated security offered by the protocol
    itself, platform independence, scalability, and the ability to perform data modeling,
    meaning that more complex structures can be managed depending on the context and/or the
    company implementing the protocol.

    \item[PLC]
    An industrial computer that has been ruggedized and adapted for the control of manufacturing
    processes, such as assembly lines, machines, robotic devices, or any activity that requires
    high reliability and usually works in real time.

    \item[OEE - Overall Equipment Effectivness]~\cite{oee_wikipedia}
    Main KPI for measuring the production capacity of a manufacturing company. It is often used
    in Lean Manufacturing to achieve operational excellence.

    \item[Tag]
    OPC UA node or, generally speaking, a variable that could be collected, exposed or saved.

    \item[HUB Node] 
    Node of the distributed system provided by this software that collects data from OPC UA servers,
    stores it locally, and exposes it via REST API. It participates in the
    distributed synchronization protocols described below.
    
    \item[Anti-Entropy] 
    Synchronization protocol that guarantees eventual consistency
    through periodic exchanges of state differences between nodes.
    
    \item[Gossip Protocol] 
    Peer-to-peer communication protocol in which each node periodically propagates
    information about its own health and that of other
    observed nodes, enabling distributed failure detection.
    
    \item[Lamport Clock] 
    Logical timestamp that establishes a causal order between events
    in a distributed system without requiring physical clock synchronization.
    
    \item[Eventual Consistency] 
    Consistency model in which, given sufficient time and the absence of new
    writes, all nodes in the system will converge to the same state.

    \item[Load Balancer] 
    Component that distributes client requests among available HUB nodes,
    implementing health checks to detect non-functioning nodes.

    \item[HTTP - HyperText Transfer Protocol]~\cite{rfc2616}
    Fundamental protocol for exchanging data on the World Wide Web. Based on a client-server
    model, it allows clients(browser as an example) to request and receive resources.

    \item[TLS - Transport Layer Security]~\cite{krawczyk2016optls}
    Cryptographic protocol designed to secure communications over a computer network.
    It is widely used in applications such as email, instant messaging
    and Voice over IP (VoIP), but it's best known for securing Hypertext Transfer Protocol
    Secure (HTTPS).
    There are different versions, and the most recent one is currently 1.3. However, different
    legacy versions are widely used and leave security issues all over the internet.
    
    \item[JWT (JSON Web Token)]~\cite{rfc7519}
    Is an open standard (RFC 7519) that defines a compact, self-contained way
    to securely transmit information between parties as a JSON object. It is commonly used
    for authentication and authorization, allowing servers to verify user identity without
    database lookups. These tokens are digitally signed for security, ensuring data integrity.
    
    \item[RBAC (Role-Based Access Control)]~\cite{ferraiolo1995role}
    Security method that regulates access to system resources based on
    defined roles rather than individual user identities.
    Assigns specific permissions to roles (e.g., admin, user, manager), simplifying security management.
    
\end{description}

\subsection{Functional Requirements}\label{functional-requirements}

\begin{enumerate}
    \item \textbf{OPC UA Server Connection}
    
    The system must connect to configured OPC UA servers and periodically read the values
    of the defined variables (OPC UA nodes).
    
    \textit{Acceptance Criterion}: Each HUB node can connect to one or more
    OPC UA servers, authenticate correctly, and read values at a configurable frequency
    (default: every 5 seconds). The data read includes the OPC UA server timestamp
    and status code.
    
    \item \textbf{Data Persistence}
    
    The system must store the data collected from the OPC UA servers locally
    in a relational database on each HUB node.
    
    \textit{Acceptance Criterion}: The data are saved in a structured format
    (lamport\_clock, tag, node\_id, value, timestamp, quality, source\_server, server\_name)
    with support for temporal queries and the db itself persists even after the node is restarted.
    
    \item \textbf{REST API Data Exposure}
    
    The system must expose the collected data through authenticated REST APIs,
    allowing clients to query current and historical values.
    
    \textit{Acceptance Criterion}: API for data retrieve:
    \begin{itemize}
        \item GET \texttt{/api/v1/tags} - list all the tags for all the OPC UA server
        \item GET \texttt{/api/v1/servers/{server\_name}/tags} - list all the tags for a specific OPC UA server source
        \item GET \texttt{/api/v1/servers/{server\_name}/tags/{tag}/latest} - retrieve last value for a specific tag that is exposed in one specific OPC UA server
        \item GET \texttt{/api/v1/servers/{server\_name}/tags/{tag}/history} - retrieve value for a specific tag that is exposed in one specific OPC UA server into a given time range
        \item GET \texttt{/api/v1/servers} - list all configured OPC UA servers 
    \end{itemize}
    Every HTTP request need valid JWT into the Authorization header.
    
    \item \textbf{Dynamic Server Addition}
    
    The system must allow the addition of new OPC UA servers without the need to restart the HUB node.
    
    \textit{Acceptance Criterion}: An admin user can invoke
    HTTP POST request at \texttt{/api/v1/admin/opc-servers} path including new OPC UA server
    configuration including a name for the source and the endpoint on which the data will be
    retrieved. The system immediately starts collecting data from the
    new server and propagates the configuration to the other HUB nodes via
    Anti-Entropy. The maximum propagation time is 30 seconds.
    
    \item \textbf{Data Synchronization Between Nodes}
    
    The system must synchronize the data collected between the three (or more) HUB nodes to
    ensure that each node has a consistent copy of the data.
    
    \textit{Acceptance Criterion}: The Anti-Entropy protocol periodically compares
    the state between nodes every 10 seconds using Lamport clocks. Differences are resolved
    using the Last-Write-Wins strategy. After 30 seconds without new writes, all nodes must converge
    to the same state.
    
    \item \textbf{Failure Detection}
    
    The system must automatically detect when a HUB node becomes unreachable.
    
    \textit{Acceptance Criterion}: The Gossip protocol implements heartbeat
    every 5 seconds between nodes. A node is considered “suspected failed”
    after 3 consecutive missed heartbeats (15 seconds). The information is
    propagated to other nodes via Gossip with a detection probability
    of $>$95\% within 20 seconds.
    
    \item \textbf{Automatic Traffic Redirection}
    
    The system must automatically redirect client traffic from non-functioning nodes to operational nodes.
    
    \textit{Acceptance Criterion}: The nginx load balancer performs HTTP health checks
    every 30 seconds on the /health endpoint of each HUB node. If a node
    does not respond or returns a 500 status, it is automatically removed from the
    routing pool. Recovery occurs automatically when the node
    returns to operation.
    
    \item \textbf{User Authentication}
    
    The system must authenticate users via credentials and issue JWT tokens for API access.
    
    \textit{Acceptance Criterion}: The POST endpoint \texttt{/api/v1/auth/token} accepts username and
    password, verifies the credentials, and returns a JWT valid for 24 hours. The token includes
    the user's role (admin or user).
    
    \item \textbf{Role-Based Authorization}
    
    The system implements role-based access control.
    
    \textit{Acceptance Criterion}:
    \begin{itemize}
        \item Ruolo \texttt{user}: can only read data;
        \item Ruolo \texttt{admin}: can read and modify configurations.
    \end{itemize}
    Unauthorized requests return HTTP 403 Forbidden.

\end{enumerate}

\subsection{Non-Functional Requirements}\label{non-functional-requirements}

\begin{enumerate}
    \item \textbf{Eventual Consistency}
    
    The system prioritizes availability and partition tolerance over strong consistency, implementing eventual consistency.
    
    \textit{Acceptance Criterion}: During network partitions or partial failures, nodes continue
    to accept writes (server addition) and reads. Convergence to the same state is guaranteed
    within 60 seconds of connectivity restoration. Different clients may observe temporarily different
    snapshots.
    
    \item \textbf{Response Time}
    
    REST APIs must respond with acceptable latency for industrial monitoring applications.
    
    \textit{Acceptance Criterion}:
    The following tests have been performed in a LAN(Local Area Network). So, since the system
    is created to work also in different networks, other tests would be performed to estimate
    reponse time in the specific case.
    The first two items of the following list have been tested under a load of 50 requests per
    second distributed across the three nodes.
    \begin{itemize}
        \item Live data reading: P95 latency $<$ 100ms
        \item Historical data reading: P95 latency $<$ 1s for 1000 record
        \item Aggiunta server: $<$ 100ms (local processing + asynchronous propagation)
    \end{itemize}
    
    \item \textbf{Data Integrity}
    
    The collected data must be stored with guaranteed integrity,
    preserving the original timestamps of the OPC UA server.
    
    \textit{Acceptance Criterion}: Data is never modified after initial entry. Each record
    includes the timestamp of the source OPC UA server, allowing for correct temporal sorting even in the
    presence of clock drift between HUB nodes.
    
    \item \textbf{Scalability - Server OPC UA}
    
    The system must support a growing number of OPC UA servers without
    significant performance degradation.
    
    \textit{Acceptance Criterion}: The system must manage a number of OPC UA servers as required by
    the end user. Consider scaling out the HUB nodes, trying to
    maintain a 1:10 ratio between HUB nodes and OPC UA servers, obviously always leaving a
    minimum of 3 HUB nodes.
    
    % TODO va messo poi sotto nell'apposita sezione
    \item \textbf{Security - Data in Transit}
    
    All communications between the client and the system must be encrypted.
    
    \textit{Acceptance Criterion}: The nginx load balancer implements TLS 1.3~\cite{krawczyk2016optls} termination with
    valid certificates, even if in this case they're provided as self-signed, since a CA authority
    request could be submitted for receive a valid one that would be recognized and verifiable
    all over the internet.
    Unencrypted HTTP connections are redirected directly to HTTPS.
    If the system is to be published on a public domain, tests like on
    \emph{www.ssllabs.com/ssltest} site are recoomend to ensure a high level of security.
        
    \item \textbf{Security - Authentication}
    
    The system must prevent unauthorized access to APIs.
    
    \textit{Acceptance Criterion}: Every request to the APIs (except /auth/login)
    requires a valid JWT. Expired, malformed, or invalidly signed tokens are rejected with HTTP
    401. The JWT secret is shared among all HUB nodes and it's auto-generated during 
    application setup.
    
\end{enumerate}

\subsection{Implementation Requirements}\label{implementation-requirements}

\begin{enumerate}
    \item \textbf{Programming Language: Python}
    
    The backend of the HUB nodes must be developed in Python 3.11+.
    
    \textit{Justification}: Python~\cite{python} offers recent libraries for OPC UA
    (asyncua), high-performance web frameworks with asynchronous support
    (FastAPI), and a rich ecosystem for rapid development. The I/O-bound nature
    of the system (network + database) benefits from Python's asynchronous event loop.
    
    \item \textbf{Web Framework: FastAPI}
    
    REST APIs must be implemented with FastAPI.
    
    \textit{Justification}: FastAPI~\cite{fastapi} provides high performance comparable
    to Node.js thanks to Starlette and uvicorn, automatic data validation
    with Pydantic, automatic OpenAPI documentation generation, and native support
    for asynchronous operations essential for non-blocking I/O to
    OPC UA servers and databases.

    Website for details: https://fastapi.tiangolo.com/.

    \item \textbf{ORM: SQLAlchemy}
    
    Access to the database must use SQLAlchemy as the ORM.
    
    \textit{Justification}: SQLAlchemy~\cite{sqlalchemy} is the standard ORM in Python, providing
    database abstraction (SQLite in development, PostgreSQL in
    production possible), support for schema migrations via Alembic,
    and type-safe queries that reduce runtime errors.

    \item \textbf{Database: SQLite}
    
    Each HUB node uses SQLite as its local database.
    
    \textit{Justification}: SQLite~\cite{sqlite} is embedded, does not require a separate server,
    has a minimal footprint, and is sufficient for expected workloads
    (write-intensive but not concurrent between different processes). Each node
    operates on an independent database, replicated via Anti-Entropy.

    \item \textbf{Data validation and serialization: Pydantic}

    LData validation and serialization must be implemented using Pydantic.

    \textit{Justification}: Pydantic~\cite{pydantic} provides automatic data validation using
    Python type hints, ensuring data integrity at the API boundaries. It integrates
    seamlessly with FastAPI for automatic validation of requests and responses, generates
    JSON schemas for API documentation (see below: Swagger API), and prevents common errors
    such as type mismatches or missing required fields.
    Pydantic define clear models for:
    \begin{itemize}
        \item request/response API payload (es. server config, authentication credentials)
        \item configuration file validation (startup parameter, JWT secret, ...)
        \item database model with SQLAlchemy integration
    \end{itemize}
    This approach provides type safety at runtime, catching errors before they propagate through the system, and reduces the need for manual validation code.
    For example, when adding a new OPC UA server via POST \texttt{/api/v1/admin/opc-servers},
    Pydantic automatically validates that the endpoint URL is well-formed, that the required fields
    are present, and that the data types are correct, returning clear error messages
    to the client in case of failed validation.
    
    
    \item \textbf{Containerization: Docker}
    
    The system must be deployable via Docker and Docker Compose.
    
    \textit{Justification}: Docker~\cite{docker} guarantees environment reproducibility, simplifies multi-node
    deployment on a single machine for development/testing, isolates dependencies,
    and make scale out easier. Docker Compose orchestrates the 3(or even more) simulated
    HUB + nginx + OPC UA server(to simulate real servers) nodes with a single command.
    
    \item \textbf{Load Balancer: nginx}
    
    Load balancing and TLS termination must be handled by nginx.
    
    \textit{Justification}: Nginx~\cite{nginx} is extremely powerful for reverse
    proxy, supports native health checks, simple configuration for
    round-robin, and robust TLS management. Extensively tested in production
    and with minimal footprint.
    
    \item \textbf{OPC UA Library: asyncua}
    
    The connection to OPC UA servers must use the asyncua library.
    
    \textit{Justification}: Asyncua~\cite{asyncua} is a pure Python implementation of OPC UA client with
    native asynchronous support, compatible with FastAPI event loop, actively maintained,
    and supports necessary OPC UA features (browsing, subscriptions, read multiple nodes).

    Website for details: https://github.com/FreeOpcUa/opcua-asyncio
    
    \item \textbf{Authentication JWT: python-jose}
    
    Authentication must implement JSON Web Tokens by integrating it with Python.
    
    \textit{Justification}: Python-Jose~\cite{pythonjose} is a popular Python library for working with JWT, which
    support various algorithms and allows the creation, decoding, and verification of tokens,
    ideal for secure authentication.

    Website for details: https://python-jose.readthedocs.io/en/latest/.
    
    \item \textbf{Development Environment: PowerShell Script or Bash Script}
    
    Setup and authentication scripts must be provided in PowerShell and Bash.
    
    \textit{Justification}: since the operating system on which the software will run is not
    defined, two setup scripts are provided, one in PowerShell and one in Bash, allowing
    the user to run the script that best suits their needs.
    Within the script, TLS certificates are automatically generated, or possibly recreated if
    they are detected but expired, and the user is prompted to enter the username and password
    of the admin user, in addition to the option of creating additional users who will be
    classified as normal users (see previous section - RBAC).
    The information provided by the user will be entered into an environment variables file.

\end{enumerate}



\subsection{Relevant Distributed System Features}\label{ds-features}

The following analysis explains which characteristics of distributed systems are
critical to the success of the project and which are less relevant in the specific context
of an industrial data collection system.

\subsubsection{Fault Tolerance e Dependability}

\textbf{Relevance: HIGH} - Fault tolerance is the fundamental pillar of the system, given that industrial plants
require continuous and constant monitoring.

\begin{itemize}
    \item \textbf{Availability}: The system must guarantee high availability since the final user, like the customer
    that could buy it, would track all the machine status in order to calculate machine
    performance and quality, introducing OEE calculation concept. What’s more,
    most of the company the buy industrial automation machine, works 24/7 and
    need continuos monitoring about machines’ state.
    With three(at least) operational HUB nodes, the system tolerates the
    failure of one or two nodes while continuing to serve requests without interruption.
    As the number of HUB nodes increases, the fault tolerance capacity clearly grows.
    An assessment must therefore be made to understand how many requests will be
    made to the distributed system and how frequent they will be, so that the number
    of nodes can be scaled to a sufficient number.

    \item \textbf{Partion Tolerance}
    The system must continue to work correctly even in the presence of network par-
    titions between HUB nodes.
    If the network has partitions and nodes are splitted into isolated groups, each group
    continues to serve clients connected to it. When the partition resolves, the Anti-Entropy
    protocol automatically reconciles divergent states.
    
    \item \textbf{Reliability}: Data loss is unacceptable in an industrial context,
    where each sample represents the status of a machine at a specific moment in time.
    Replication via Anti-Entropy ensures that the data collected by one node
    is propagated to all other nodes, preventing data loss
    even in the event of a sudden crash.
    
    \item \textbf{Recovery Time}: The system implements automatic recovery mechanisms:
    \begin{itemize}
        \item Gossip protocol detects failure within $\sim$15 seconds (3 heartbeat missed).
        \item Load balancer remove not working nodes from pool in $\sim$30 seconds.
        \item Anti-entropy resynchronizes restored nodes in $<$60 seconds.
    \end{itemize}
    
    \item \textbf{Integrity}: Each piece of data includes the original OPC UA timestamp and
    Lamport clock for causal ordering, ensuring that temporal integrity is preserved even with
    clock drift between nodes.
    
\end{itemize}

\subsubsection{High Availability}

\textbf{Relevance: HIGH} - High availability is not only desirable but necessary for industrial use cases:

\begin{itemize}
    \item \textbf{No Single Point of Failure}: Each component is redundant:
    \begin{itemize}
        \item 3 independent HUB nodes collect data in parallel.
        \item Each node has a local database (no dependency on a central database).
        \item Load balancer can be replicated (not implemented in this prototype
        but architecturally possible).
    \end{itemize}
    
    \item \textbf{Partition Tolerance}: During network partitions, each isolated group
    continues to operate autonomously. When the partition resolves, Anti-Entropy automatically
    reconciles divergent states.

\end{itemize}

\subsubsection{Transparency}

\textbf{Relevance: HIGH} - Transparency is essential for simplifying the use of the system:

\begin{itemize}
    \item \textbf{Location Transparency}: clients interact with a single endpoint
    (the load balancer) without knowing which HUB node is actually serving the request.
    This hides the physical distribution and allows horizontal scaling
    by adding nodes without changing client configurations.
    
    \item \textbf{Replication Transparency}: clients are unaware that the data
    is replicated across three nodes. They send a single GET request and receive a
    response, regardless of which replica serves the request.
    
    \item \textbf{Failure Transparency (parziale)}: HUB node failures are
    completely transparent to clients: requests continue to be served
    by the surviving nodes. However, eventual consistency implies that different clients
    may temporarily observe slightly different states during synchronization windows,
    so failure transparency is not total.
    
    \item \textbf{Access Transparency}: REST APIs are identical regardless of the node
    on which the request is forwarded,ensuring a uniform interface.

\end{itemize}

\subsubsection{Scalability}

\textbf{Relevance: HIGH} - Scalability is essential to adapt to the growth of the industrial plant:

\begin{itemize}
    \item \textbf{Horizontal Scalability - HUB nodes}: HUB nodes can be added
    to the cluster to further distribute the load. The system is designed to be stateless at the
    level of individual HTTP requests (authentication via stateless JWT), facilitating the
    addition of new nodes.
    
    \item \textbf{Horizontal Scalability - OPC UA server}: New OPC UA servers can
    be added dynamically via admini APIs without restarting. The configuration is automatically
    propagated to all HUB nodes via Anti-Entropy.
    
    \item \textbf{Request Scalability}: The load balancer distributes requests
    in round-robin fashion. By adding a fourth node, the theoretical throughput increases by 33\%.
    
    \item \textbf{Storage Scalability}: Each node stores all collected data locally.
    For very large datasets, retention strategies can be implemented (e.g., keep the last 30
    days locally, archive historical data on separate storage).

\end{itemize}

\subsubsection{Security e Trust}

\textbf{Relevance: HIGH} - Security is critical since the system exposes potentially sensitive data from production processes:

\begin{itemize}
    \item \textbf{Authentication}: JWT-based authentication ensures that only
    authenticated users can access the APIs. Tokens expire (24 hours)
    and include role information for authorization.
    
    \item \textbf{Authorization (RBAC)}: Role-based access control:
    \begin{itemize}
        \item Utenti \texttt{user}: data read-only permissions.
        \item Utenti \texttt{admin}: can modify configurations by adding OPC UA server.
    \end{itemize}
    
    \item \textbf{Data in Transit}: TLS 1.3 termination on nginx encrypts all
    client-system communications. Unencrypted HTTP connections are rejected
    or redirected.
    
    \item \textbf{Trust Model}: The system assumes that:
    \begin{itemize}
        \item HUB nodes are trusted (they operate in the same controlled environment).
        \item OPC UA servers are trusted (part of the industrial infrastructure).
        \item Clients are untrusted (require authentication/authorization).
    \end{itemize}
    
    \item \textbf{Secret Management}: The JWT secret is automatically generated during
    setup and shared between HUB nodes via configuration. In production, we recommend
    using secret management tools (e.g., HashiCorp Vault).
\end{itemize}

\subsubsection{Resource Sharing e Coordination}

\textbf{Relevance: MEDIUM-HIGH} - Multiple components share resources and require coordination:

\begin{itemize}
    \item \textbf{Shared Resource: OPC UA Servers}: Multiple HUB nodes can
    connect to the same OPC UA servers in read mode. There is no conflict because
    the operations are read-only (the system does not send control commands to the
    machines).
    
    \item \textbf{Coordination for Configuration}: When an administrator adds a new
    OPC UA server, the configuration must be propagated to all nodes. Anti-entropy
    with Lamport clocks ensures consistent ordering of configuration events.
    
    \item \textbf{No Strong Synchronization}: Unlike systems that require
    strong consensus (e.g., Raft, Paxos), this system deliberately avoids synchronous
    consensus protocols in favor of availability. Coordination is asynchronous and best-effort.
    
    \item \textbf{Conflict Resolution}: The Last-Write-Wins strategy based on Lamport clocks
    resolves conflicts without voting or synchronous coordination.
    The Last-Write-Wins strategy based on Lamport clocks resolves conflicts without voting or
    synchronous coordination.

  \end{itemize}

\subsubsection{Openness, Interoperability, Heterogeneity}

\textbf{Relevance: HIGH} - Interoperability is essential since the system is a middleware that integrates different
protocols:

\begin{itemize}
    \item \textbf{OPC UA protocol(Input)}: open industry standard for
    communication with PLCs, SCADA, and DCS from any vendor. Ensures interoperability
    with existing industrial ecosystems (Siemens, Schneider Electric, Rockwell, etc.).
    
    \item \textbf{REST API(Output)}: modern web standard for exposing data. It allows
    integration with business intelligence applications, web dashboards, ERP systems,
    mobile apps, etc.
    
    \item \textbf{Standardization}: the use of standard protocols (OPC UA, HTTP/REST,
    TLS, JWT) ensures that the system can be integrated into heterogeneous architectures
    without vendor lock-in.
    
    \item \textbf{Heterogeneity of Components}: the system integrates:
    \begin{itemize}
        \item OPC UA server;
        \item HUB Python nodes (FastAPI, SQLAlchemy);
        \item Nginx load balancer (C);
        \item HTTP client, every languae or platform.
    \end{itemize}
    
    \item \textbf{Openness}: Open architecture allows for future extensions:
    \begin{itemize}
        \item addition of alternative input protocols (MQTT, Modbus, Profinet);
        \item addition of alternative output protocols (GraphQL, gRPC, WebSocket);
        \item integration with analytics systems (InfluxDB, TimescaleDB).
    \end{itemize}
\end{itemize}

\subsubsection{Evolvability e Maintainability}

\textbf{Relevance: HIGH} - The system is designed for long-term evolution:

\begin{itemize}
    \item \textbf{Containerization}: Docker ensures consistent deployment and
    simplifies updates. Each component (HUB node, nginx, OPC UA simulator)
    is containerized independently.
    
    \item \textbf{Rolling Updates}: Ability to update nodes one at a time
    without total downtime, which is essential for 24/7 systems.
    
    \item \textbf{API Versioning}: The APIs include versioning in the path (\texttt{/api/v1/...})
    allowing the introduction of incompatible versions while maintaining backward compatibility.
    
    \item \textbf{Configuration Management}: Configuration outsourced to
    \texttt{.env} files and automatically generated by setup scripts. Configuration changes
    do not require recompilation.
    
    \item \textbf{Monitoring e Debugging}: Endpoint \texttt{/health} for health checks,
    structured logging, and the possibility of adding Prometheus/Grafana metrics
    in the future.
    
    \item \textbf{Modular Architecture}: Well-separated components:
    \begin{itemize}
        \item OPC UA Ingestors (data collection)
        \item API layer (data exposure)
        \item Gossip agent (failure detection)
        \item Anti-entropy agent (synchronization)
    \end{itemize}
    They allow isolated changes to individual modules even if they're integrated into the HUB nodes.
    See the design part for more info.
\end{itemize}

\subsubsection{Performance, Concurrency, Communication Efficiency}

\textbf{Relevance: MEDIUM} - Performance is important but not as critical as availability:

\begin{itemize}
    \item \textbf{Acceptable Latency}: for industrial monitoring, latencies
    P95 of 100-200ms are acceptable. The system is not real-time critical (it does not
    control machinery, it only collects data).
    
    \item \textbf{Concurrency}: FastAPI with asynchronous event loop handles multiple
    concurrent requests without blocking.
    
    \item \textbf{Bandwidth}: for data collection, it depends strictly on the number
    of variables present on the OPC UA servers. As the number of variables increases,
    the bandwidth used increases linearly.
    As for user requests, it all depends on how many applications use
    the system to retrieve data.
    
    \item \textbf{Network Efficiency}: Anti-Entropy exchanges only differences (delta),
    not the entire dataset.
    
    \item \textbf{Tradeoff Awareness}: The system sacrifices strong consistency
    (which would require synchronous coordination and greater latency) to achieve
    greater throughput and availability.
\end{itemize}

\subsubsection{Economy e Costs}

\textbf{Relevance: MEDIUM} - Some economic considerations can influence architectural choices:

\begin{itemize}
    \item \textbf{Open Source Stack}: all technologies used are open source
    (Python, FastAPI, SQLite, nginx, Docker), eliminating licensing costs.
    
    \item \textbf{Operational Costs}: containerized deployment reduces operational complexity.
    Setup scripts automate configuration, reducing deployment and maintenance costs.
    
    \item \textbf{Scalability Costs}: obviously adding capacity for allowing application scale update
    requires new nodes as hardware commodities, and the cost increase would depends on which
    hardware will be selected and on how much the system has to grow.

\end{itemize}

\subsubsection{NOT relevant or Secondary Features} - For the sake of completeness, some characteristics of distributed systems are of lesser relevance
to this project:

\begin{itemize}
    \item \textbf{Strong Consistency}: Deliberately NOT implemented due to the AP priority.
    
    \item \textbf{Consensus Protocols}: Protocols such as Raft or Paxos are not
    necessary given the nature of the data (time-series append-only without conflicting
    concurrent writes) and for development time constraints.
    
    \item \textbf{Byzantine Fault Tolerance}: Not necessary since all nodes
    operate in a controlled environment and are trusted.
    
    \item \textbf{Real-Time Guarantees}: The system has no hard real-time requirements.
    Latencies in the order of hundreds of milliseconds are acceptable.
    
    \item \textbf{Geo-Distribution}: In this prototype, nodes are co-located
    (same LAN). In production, it could be extended to geo-distributed deployment as said at the
    beginning of this report, but this is not a current requirement.

\end{itemize}

\clearpage

\section{Design}\label{design}

This section show the architectural strategies adopted to meet the requirements
identified in the analysis. The design choices are justified in relation to the requirements
defined in \cref{requirements} and the characteristics of the distributed systems analyzed
in \cref{ds-features}.

\subsection{Architecture}\label{architecture}

\subsubsection{Architectural Style}

The system adopts a hybrid architectural style:

\begin{itemize}
\item \textbf{Client-Server Layer}:
External applications access the system via REST API
\begin{itemize}
    \item \textbf{Client}: Dashboards, ERP systems, and mobile apps require data via HTTPS.
    \item \textbf{Server}: HUB nodes expose authenticated REST APIs
    \item \textbf{Load Balancer}: nginx masks the physical distribution (location transparency)
\end{itemize}

\item \textbf{Peer-to-Peer Layer}:
HUB nodes synchronize via peer-to-peer communication
\begin{itemize}
    \item No central master/coordinator
    \item Internal HTTP P2P communication (\texttt{/internal/*} path)
    \item Static discovery via configuration
\end{itemize}
\end{itemize}


\subsubsection{Reasons}

This hybrid architecture offers some advantages:

\begin{itemize}
    \item\textbf{Client Simplicity}: load balancer hides distribution because clients see a single endpoint;
    
    \item\textbf{High Availability}: no master eliminates singe point of failure and each node can serve every request;
    
    \item\textbf{Horizontal Scalability}: adding nodes only requires updating the nginx configuration;
        
    \item\textbf{Fault Isolation}: problems on one node do not propagate and each node operates independently.
\end{itemize}

\clearpage

\subsection{Infrastructure}\label{infrastructure}

\subsubsection{Infrastructural Components}

\begin{itemize}
\item\textbf{HUB Nodes (3+ instances)}
Core nodes of the distributed system. Each HUB:
\begin{itemize}
    \item collects data from OPC UA servers (OPC UA client - OPC UA Ingestor);
    \item exposes REST APIs (HTTP server);
    \item synchronize with other nodes (peer-to-peer data exchange);
    \item maintains local SQLite database with eventual consistency replication;
    \item performs Gossip (failure detection) and Anti-Entropy (synchronization).
\end{itemize}

Minimum 3 nodes to tolerate failure of 2 nodes and this is linearly scalable.

\item\textbf{Load Balancer (1 nginx instance)}
It's a reverse proxy that:
\begin{itemize}
    \item distributes requests in round-robin fashion;
    \item terminate TLS (port 443);
    \item health checks HTTP every 30 seconds on \texttt{/api/v1/health} path
    \item automatically removes unresponsive nodes
\end{itemize}


\item\textbf{OPC UA Servers (N instances)}
External industrial systems (PLCs, SCADAs, or simulators) that expose process variables.
From the distributed system's point of view, these are untrusted external entities
with potentially unreliable connectivity. HUB nodes handle this via connection retry logic.
The system allows dynamic addition of servers at runtime: when administrators add
servers via POST to \texttt{/api/v1/admin/opc-servers} and the configuration propagates
to peers via Anti-Entropy.
This software provides different OPC UA server simulator that will be useful to test all
the system if others OPC UA servers are not availables.
\begin{itemize}
    \item display hierarchical OPC UA tree nodes;
    \item variables(or \emph{tags}) represent machine states (temperatures, pressures, counters)
    \item external to the system (untrusted)
\end{itemize}

\item\textbf{HTTP Clients (N instances)}
These could be potentially every tool that has an HTTP client, like web page dashboard, ERP,
analytics tools or mobile app.
\end{itemize}

\subsubsection{Network Distribution}

\begin{itemize}
\item \textbf{Development/Test}: All components on a single machine as Docker containers
with a private network bridge.
\item \textbf{Production}: HUB nodes geographically distributed across different availability zones,
replicable load balancer with DNS load balancing, OPC UA servers physically located in the plants,
for example in the PLC that manage the machine.
\end{itemize}

\subsubsection{Component Discovery}

This subsection show how components are aware of other components.

\begin{itemize}
\item \textbf{Client - Load Balancer}: Its use in production will involve DNS resolution, but, as
is also the case in test and development, its IP address will be utilised.

\item \textbf{Load Balancer - HUB Nodes}: Static config \texttt{nginx.conf} and health checks monitoring.

\item \textbf{HUB - HUB}: Peers list configured in environment variables, failure detection via Gossip.

\item \textbf{HUB - OPC UA}: Dynamic config via \texttt{POST /api/v1/admin/opc-servers}, saved into DB and replicated via Anti-Entropy.
\end{itemize}

\subsubsection{Infrastructure Diagram}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/infrastructure-diagram.png}
  \caption{
    Clients access via nginx, which distributes to HUB nodes.
    The nodes synchronize P2P (Gossip and Anti-Entropy) and collect data from OPC UA servers
  }
  \label{fig:infrastructure}
\end{figure}

\clearpage

\subsection{Modelling}\label{modelling}

\subsubsection{Domain Entities}

\begin{itemize}
\item \textbf{OPCUADataPoint}

Single tag sample. Each instance is created by the Ingestor into the HUB node that collects data
from the OPC UA servers. Each one is immutable after creation since Lamport clock guarantees total ordering.
\begin{itemize}
    \item \textbf{tag}: legible name (es. "Temperature.Oven1");
    \item \textbf{node\_id}: OPC UA node id;
    \item \textbf{value}: numeric/boolean/string value;
    \item \textbf{timestamp}: UTC timestamp from the OPC UA server;
    \item \textbf{quality}: GOOD/BAD/UNCERTAIN (OPC UA standard);
    \item \textbf{source\_server}: source endpoint:
    \item \textbf{server\_name}: server name identifier;
    \item \textbf{lamport\_clock}: logical timestamp for causal ordering.
\end{itemize}

\item \textbf{ServerConfig}
OPC UA server configuration:
\begin{itemize}
    \item \textbf{server\_name}: server name identifier;
    \item \textbf{endpoint}: OPC UA server URL;
    \item \textbf{lamport\_clock}: logical timestamp for causal ordering;
    \item \textbf{node\_id}: HUB node that received the addition request;
\end{itemize}

This config will be replicated via Anti-Entropy and is the entity that allows dynamic addition without restarting.

\item \textbf{User}
System user (stored in env, not DB):
\begin{itemize}
    \item \textbf{username};
    \item \textbf{password\_hash};
    \item \textbf{role}: admin/user.
\end{itemize}

\item \textbf{NodeInfo}
Peer metadata used by Gossip protocol:
\begin{itemize}
    \item \textbf{node\_id}, \textbf{host}, \textbf{port}
    \item \textbf{is\_alive}: Gossip flag;
    \item \textbf{last\_seen}: last heartbeat timestamp;
    \item \textbf{lamport\_clock}: last known LC.
\end{itemize}

\end{itemize}

\subsubsection{Entity Mapping to Infrastructure}

Pattern: \textbf{Local Storage with Peer Replication}

\begin{itemize}

\item \textbf{OPCUADataPoint}: Created by OPC UA Ingestor, saved in local SQLite
(\texttt{data\_points} table) and replicated via Anti-Entropy. Potentially queried by REST API.

\item \textbf{ServerConfig}: Created via POST admin API, stored in SQLite
(\texttt{server\_configs}), replicated via Anti-Entropy and used by Ingestor.

\item \textbf{User}: Stored in env variables, verified by JWT middleware. It's not replicated.

\item \textbf{NodeInfo}: Maintained by Gossip agent. This is in-memory and volatile and it's rebuilt at restart

\end{itemize}

\textbf{Reason}: Local database per node eliminates SPOF, minimizes latency, guarantees
operational autonomy.
\textbf{Cost}: eventual consistency instead of strong consistency.

\subsubsection{Domain Events}

\begin{itemize}
    \item \textbf{OPCUADataCollected}: generated every 5 seconds, batch save trigger + Lamport increment;
    \item \textbf{ServerAdded}: generated by POST admin, trigger save DB + connect OPC UA + propagation;
    \item \textbf{NodeSuspectedDead}: generated after 3 missed heartbeats (15s);
    \item \textbf{AntiEntropySync}: generated every 10 seconds, LC comparison trigger + data pull.
\end{itemize}

\subsubsection{Message Types}

\begin{itemize}
\item \textbf{Client - HUB (REST Commands)}:
\begin{itemize}
  \item\textbf{LoginRequest}: user credentials (username and password) to obtain JWT authentication tokens,
  sent as POST to \texttt{/api/v1/auth/token} with form data;
  \item\textbf{AddOPCServerRequest}: allows administrators to dynamically add OPC UA servers, containing
  server name and endpoint, sent as POST to \texttt{/api/v1/admin/opc-servers} with
  admin authentication;
  \item\textbf{DataQuery}: represents requests to read industrial data
  via GET on endpoints such as \texttt{/api/v1/servers/\{name\}/tags/\{tag\}/latest} or
  \texttt{/history}, including query parameters for time filtering and result limits.
   
\end{itemize}

\item \textbf{HUB - Client (REST Responses)}:
\begin{itemize}
  \item \textbf{TokenResponse (JWT)}: contains signed JWT plus metadata (token type "bearer", 
  user role, expiration 24h), allowing clients to authenticate subsequent requests
  \item \textbf{DataPointResponse}: single industrial sample with tag, value, timestamp, 
  quality status, Lamport clock, and server identifiers
  \item \textbf{HistoryResponse}: contains array of data points for requested time range 
  plus metadata (count, serving node\_id)
\end{itemize}

\item \textbf{HUB - HUB (P2P Internal HTTP)}:
\begin{itemize}
  \item \textbf{GossipMessage}: Implements failure detection with two variants:
  “ping” sent every 5 seconds by the sender with its own Lamport clock to signal liveness, and
  “ack” responded by the recipient with its own Lamport clock to confirm receipt.
  Failure to receive ack within 3 seconds indicates a potentially unreachable peer;
  \item \textbf{AntiEntropyRequest}: start data synchronization by transporting the range
  Lamport clock possessed by the requester (min\_lc, max\_lc), allowing the peer to
  identify gaps;
  \item \textbf{AntiEntropyResponse}: contains missing data points for the requester
  (LC > max\_lc requester) plus current\_max\_lc of the responder, enabling incremental convergence;
  \item \textbf{ServerConfigSyncRequest/Response}: follow the same pattern but
  work on OPC UA server configurations instead of data points, propagating dynamic server additions
  across the cluster within 30 seconds as required by NFR-06.
\end{itemize}

\item \textbf{HUB - OPC UA (OPC UA Protocol)}:
Interaction with external industrial servers uses the standard OPC UA protocol.

\end{itemize}

\subsubsection{System State}

The global state eventually emerges through convergence: the union of all
data points sorted by LC, and the union of server configurations with Last-Write-Wins conflict resolution.
There is no “master” state; each node has a local view
that converges.

\paragraph{Per-Node State (Local)}
\begin{itemize}
    \item current Lamport Clock
    \item Data Points DB (con LC)
    \item Server Configs DB (con LC)
    \item Peer Liveness Map (volatile)
    \item OPC UA Connections
\end{itemize}

\paragraph{Global State (Eventual)}
\begin{itemize}
    \item union of all Data Points, orderer by LC. They will converge via Anti-Entropy.
    \item union of Server Configs, Last-Write-Wins on conflicts (highest LC wins).
\end{itemize}


\clearpage

\subsection{Interaction}\label{interaction}

\subsubsection{Communication Patterns}

\begin{itemize}
\item \textbf{Client-HUB: Synchronous Request-Response}
\begin{enumerate}
    \item client - nginx: HTTPS GET \texttt{/api/v1/.../latest};
    \item nginx - HUB (round-robin);
    \item HUB: verify JWT + query SQLite;
    \item HUB - nginx - Client: JSON response.
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/sequence-client-query.png}
\caption{Sequence diagram - HTTP request to retrieve OPC UA tag value}
\label{fig:seq-client-query}
\end{figure}

\item \textbf{HUB-HUB: Asynchronous P2P Gossip (Heartbeat)}:
\begin{enumerate}
    \item every 5 seconds: each HUB node creates POST \texttt{/internal/Gossip/ping} for every known peer;
    \item message include Lamport clock sender;
    \item peer responds with ACK + its own LC;
    \item 3s timeout - peer not responsive;
    \item 3 missing heartbeats (15s) - \texttt{is\_alive=False}.
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/sequence-Gossip.png}
\caption{Sequence Gossip diagram}
\label{fig:seq-Gossip}
\end{figure}

\item \textbf{HUB-HUB: Asynchronous P2P Anti-Entropy (Data Sync):}
\begin{enumerate}
    \item every 10s: every HUB node - POST \texttt{/internal/Anti-Entropy/sync} to alive peers;
    \item message: own range LC (min\_lc, max\_lc);
    \item peer query its own DB and returns data points with LC > max\_lc requester;
    \item requester insert batch + update LC = max(local, received).
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/sequence-Anti-Entropy.png}
\caption{Sequence diagram Anti-Entropy. HUB A chiede dati mancanti (LC > 500) a HUB B.}
\label{fig:seq-Anti-Entropy}
\end{figure}

\item \textbf{HUB-HUB: Asynchronous P2P Server Config Sync}:
identical to Anti-Entropy but for server\_configs. When a node receives a new configuration, it automatically start an OPC UA connection.

\item \textbf{HUB-OPC UA: Periodic Polling}
\begin{enumerate}
    \item scheduler manage a job trigger every N(5 by default) seconds for each server;
    \item job: batch read all tags (single OPC UA request);
    \item for each value
    \begin{itemize}
      \item increment LC;
      \item create DataPoint;
      \item insert into DB.
    \end{itemize}
    \item Anti-Entropy propagates automatically.
\end{enumerate}

\end{itemize}

\clearpage

\subsection{Behaviour}\label{behaviour}

\subsubsection{HUB Node State Machine - Main States}

% IMMAGINE 6: State Diagram
% \begin{figure}[H]
% \centering
% \includegraphics[width=0.85\textwidth]{figures/state-hub-node.png}
% \caption{State diagram HUB node. Transizioni tra Initializing - Connecting - Operational - Syncing.
% Degrada se perde OPC UA ma resta disponibile per client.}
% \label{fig:state-hub-node}
% \end{figure}

\begin{itemize}
    \item \textbf{Initializing}: Create/open DB, load config, initialize LC - Connecting
    \item \textbf{Connecting}: OPC UA connection + browse nodes, start Gossip/Anti-Entropy - Operational
    \item \textbf{Operational}: Client API, OPC UA polling, active Gossip/Anti-Entropy required
    \item \textbf{Syncing}: Temporary sub-state (every 10s): LC comparison, data pull, insert batch - Operational
    \item \textbf{Degraded}: OPC UA disconnected but serving client with cached data. Automatic retry - Operational
    \item \textbf{Shutdown}: SIGTERM: OPC UA Ingestion stop, Gossip stop, Anti-Entropy stop
\end{itemize}

\subsubsection{Component Responsibilities}

\begin{itemize}
    \item \textbf{FastAPI API Layer} Stateless component that exposes the system's REST interface,
    handling authentication, data queries, and response serialization. It does not maintain state
    between requests, enabling horizontal scalability.
    \begin{itemize}
      \item JWT check (HMAC-SHA256 signature and expiration);
      \item queries local SQLite with async queries;
      \item JSON serialization via Pydantic;
      \item responds to the client.
    \end{itemize}
    
    \item \textbf{OPC UA Ingestor} Stateful component responsible for collecting data from industrial servers, maintaining active connections, and managing automatic retries.
    \begin{itemize}
      \item performs batch reads every 5 seconds via a single OPC UA call;
      \item increases the Lamport clock for each value, ensuring uniqueness;
      \item creates immutable OPCUADataPoint;
      \item inserts batches into the DB;
      \item manages exponential backoff retries on failure.
    \end{itemize}
    
    \item \textbf{Gossip Agent} Stateful component that implements distributed failure detection
    among HUB peers via periodic heartbeats. Maintains peer liveness map with strong
    completeness and weak accuracy.
    \begin{itemize}
      \item send ping every 5 seconds with own Lamport clock;
      \item waits for ack with a timeout of 3s to update its last seen and LC on success;
      \item set peer as DOWN after 3 consecutive failures;
      \item responds to received pings by updating its own LC.
    \end{itemize}
    
    \item \textbf{Anti-Entropy Agent} Logically stateless component that implements
    pull-based synchronization for eventual consistency.
    \begin{itemize}
      \item queries database every 10 seconds for its own LC range;
      \item requests data from peers with LC greater than its own maximum;
      \item Inserts the received batch and updates LC to the higher of the received and local values.
      \item responds to peer requests with data where local LC is greater than the one received;
      \item synchronizes all server configurations.
    \end{itemize}
\end{itemize}

\clearpage

\subsection{Data and Consistency Issues}\label{data-and-consistency-issues}

\subsubsection{Data Storage Strategy}

The system manages two categories of data with different characteristics. The \textbf{time-series data}
(OPC UA samples) have a high volume with 17 milions records/day, assuming
1000 tags sampled every 5 seconds, with a write-heavy access pattern dominated by
continuous inserts

Data are stored in local SQLite at each node at the path
\texttt{/app/data/node-\{id\}.db}, with a schema comprising tables \texttt{data\_points}
and \texttt{server\_configs} indexed on (tag, timestamp), (tag, lamport\_clock), and
(server\_name, lc) to optimize common queries.

SQLite was chosen because it is embedded (no separate server process), requires
zero configuration and has a minimal footprint (<1MB binary). The local database per node
eliminates SPOF (no central master database), minimizes latency (all queries are
local), guarantees operational autonomy (nodes operate independently), and simplifies
deployment (no database cluster setup).

\subsubsection{Data Model}

Schema:
\begin{verbatim}
CREATE TABLE data_points (
    lamport_clock INTEGER PRIMARY KEY,
    tag TEXT NOT NULL,
    node_id TEXT NOT NULL,
    value REAL NOT NULL,
    timestamp DATETIME NOT NULL,
    quality TEXT NOT NULL,
    source_server TEXT NOT NULL,
    server_name TEXT NOT NULL
);

CREATE TABLE server_configs (
    server_name TEXT,
    lamport_clock INTEGER,
    endpoint TEXT NOT NULL,
    node_id TEXT NOT NULL,
    PRIMARY KEY (server_name, lamport_clock)
);
\end{verbatim}

\subsubsection{Query Patterns}

\begin{itemize}
    
\item \textbf{Read Queries}
\begin{itemize}
    \item Latest value: \texttt{docker exec hub-node-a sqlite3 /app/data/hub\_storage.db "SELECT * FROM data\_points WHERE tag=? AND server\_name=? ORDER BY lc DESC LIMIT 1"}
    \item History: \texttt{docker exec hub-node-a sqlite3 /app/data/hub\_storage.db "SELECT * FROM data\_points WHERE tag=? AND timestamp BETWEEN ? AND ? ORDER BY lc ASC"}
\end{itemize}

\item \textbf{Write Queries (Serialized)}
Batch insert every 5 seconds (OPC UA) + sync (Anti-Entropy) and SQLite serializes write.

\item \textbf{Conflict Handling}
Scenario: two nodes receive the same server configuration almost simultaneously.

\textbf{Resolution}: Node A assigns LC=1000, Node B assigns LC=1001. Anti-entropy propagates
both. Query takes only maximum LC - Node B “wins” (deterministic LWW).

\end{itemize}

\subsubsection{Consistency Model: Eventual Consistency}

The system implements eventual consistency: given enough time and
no new writes, all replicas converge.

\begin{itemize}

\item \textbf{Convergence Guarantees}
\begin{itemize}
    \item \textbf{Liveness}: Anti-Entropy detect and solve differences;
    \item \textbf{Convergence Time}: worst case 30s (3 × 10s sync interval), typical 10-15s;
    \item \textbf{Conflict Resolution}: Lamport clocks - total deterministic ordering.
\end{itemize}

\item \textbf{Possible Anomalies}
Clients could observe:
\begin{itemize}
    \item Read-Your-Writes violation (data on Node A not visible on Node B for 30 seconds);
    \item Monotonic Reads violation (new value then old value if switch node);
    \item Stale Reads (given with a 30-second delay).
\end{itemize}

Acceptable for industrial monitoring: data granularity 5s, 30s < human reaction time,
availability > consistency.

\end{itemize}


\clearpage

\subsection{Fault-Tolerance}\label{fault-tolerance}

\subsubsection{Data Replication: Leaderless Multi-Master}

Each node is simultaneously a source (generates events) and a replica (receives from peers).
No master/leader.

\paragraph{Replication Protocol: Pull-Based Anti-Entropy}

\begin{itemize}
\item \textbf{Pull-based Advantages}:
\begin{itemize}
    \item Natural backpressure (receiver controls rate);
    \item Sender does not track “already sent to whom”;
    \item Resilient to partitions and sync mode resumes it automatically.
\end{itemize}

\textbf{Flow}:
\item \begin{enumerate}
    \item Node A: range LC [100, 500]
    \item Node A - B: "give me LC > 500"
    \item Node B query: \texttt{SELECT * WHERE lc > 500}
    \item Node B - A: data points [501-600] + max\_lc=600
    \item Node A: insert batch + update LC = max(500, 600)
\end{enumerate}

Conflicts resolved with Last-Write-Wins (highest LC).

\end{itemize}

\subsubsection{Failure Detection: Gossip Protocol}

The Gossip protocol implements distributed failure detection through the heartbeat 
mechanism described in \cref{interaction}. The protocol provides:

\begin{itemize}

\item \textbf{Properties}
\begin{itemize}
    \item \textbf{Strong Completeness}: every failed node eventually detected by all survivors
    (if the network is eventually reliable)
    \item \textbf{Weak Accuracy}: possible false positives (slow node marked as dead),
    but automatic recovery when heartbeat resumes
\end{itemize}

\item \textbf{Error Handling}

\begin{itemize}
    \item\textbf{OPC UA Connection Failure} Retry exponential backoff (0.5s - 30s, max 20 attempts).
    Node remains operational (serves client with cached data). Alert via log.
    
    \item\textbf{Load Balancer Health Check Failure} nginx removes nodes from the pool after 1 failed check. Automatic retry, re-add when it returns to healthy status.
\end{itemize}

\end{itemize}

\clearpage

\subsection{Availability}\label{availability}

\subsubsection{Load Balancing and Failure Recovery}

The nginx load balancer described in \cref{infrastructure} implements automatic failure 
recovery through active health monitoring. When a node fails, nginx:

\begin{enumerate}
    \item Detects failure via health check (GET \texttt{/api/v1/health} returns 5xx or timeout);
    \item Removes node from routing pool after 1 failed check;
    \item Continues routing to healthy nodes without service interruption;
    \item Periodically retries failed node (every 30s);
    \item Automatically re-adds node when health check succeeds.
\end{enumerate}

This provides seamless failover with zero client-visible downtime.


\subsubsection{Behavior Under Network Partitioning}

\paragraph{Partition Scenario}
Cluster splitted: Partition 1 (Node A, B) | Partition 2 (Node C)

\begin{itemize}

\item{System Behavior}:
\begin{enumerate}
    \item \textbf{Availability Preserved}: both partitions continue OPC UA collection
    + client server + save local DB;
    
    \item \textbf{Consistency Temporarily Lost}: The eventual consistency model described 
    in \cref{data-and-consistency-issues} is temporarily violated during partition. Node C 
    does not receive data from A/B and vice versa. Clients see divergent states.
    
    \item \textbf{Failure Detection}: Partition 1 mark C dead, Partition 2 mark A/B dead (Gossip);
    
    \item \textbf{Partition Healing}: Gossip resumes - nodes alive. Automatic anti-entropy sync.
    Convergence 30-60s. Lamport clocks resolve conflicts (LWW).
\end{enumerate}

\end{itemize}

% \begin{table}[h]
% \centering
% \begin{tabular}{|l|c|l|}
% \hline
% \textbf{Property} & \textbf{Level} & \textbf{Implementation} \\
% \hline
% Consistency & Eventual & Anti-entropy + Lamport clocks \\
% Availability & High & Load balancer + independent nodes \\
% Partition Tolerance & Full & Nodes operate independently \\
% \hline
% \end{tabular}
% \caption{CAP theorem positioning: sistema AP con eventual consistency}
% \label{tab:cap-positioning}
% \end{table}

\textbf{Why AP?} Industrial monitoring does not require strong consistency. Clients prefer
data that is “30 seconds old” rather than “system unavailable.” 24/7 facilities do not tolerate downtime
by consensus.

\clearpage

\subsection{Security}\label{security}

\subsubsection{Authentication: JWT-Based}

\begin{itemize}

\item\textbf{Token Generation}
POST \texttt{/api/v1/auth/token} with username and password:
\begin{enumerate}
    \item server check credentials;
    \item JWT generations with claims (sub, role, scopes, exp=24h, iat);
    \item Token signed con JWT\_SECRET (shared secret);
    \item Response: \texttt{\{access\_token, token\_type: "bearer", role\}}.
\end{enumerate}

\item\textbf{Token Verification}
Each API endpoint requires a JWT in the \texttt{Authorization: Bearer <token>} header.
FastAPI dependency verifies signature, expiration, and claims. Invalid - HTTP 401.

\subsubsection{Authorization: RBAC}

\begin{itemize}

\item \textbf{Role: user (Read-Only)}
Permissions: GET \texttt{/servers}, \texttt{/tags}, \texttt{/.../latest}, \texttt{/.../history}, \texttt{/status}

\item \textbf{Role: admin (Read-Write)}
User permissions + POST/GET \texttt{/admin/opc-servers}

Enforcement: FastAPI dependency \texttt{verify\_admin()} check \texttt{role == "admin"} with eventual HTTP 403.

\end{itemize}

\subsubsection{Cryptographic Schemas}

\begin{itemize}
\item \textbf{TLS Encryption}
nginx use TLS:
\begin{itemize}
    \item Protocol: TLS 1.2, 1.3;
    \item ciphers: strong only (ECDHE-RSA-AES256-GCM-SHA384, etc.);
    \item certificates: self-signed (dev) o Let's Encrypt (prod);
    \item HTTP - HTTPS forced redirect. So if the some clients or user asks for HTTP instead of HTTPS they will be redirect to HTTPS. 
\end{itemize}

\item \textbf{JWT Signature}
HMAC-SHA256 with 32+ secret char (JWT\_SECRET env). Key rotation not implemented.

\item \textbf{Password Hashing}
Currently plaintext in env (demo). Production: bcrypt/argon2 con salt.

\end{itemize}

\end{itemize}


\clearpage


\section{Implementation}\label{implementation}

This section describes technology-specific choices that implement the design from
\cref{design}. The implementation leverages the Python ecosystem.

\subsection{Network Protocols}\label{network-protocols}

The selection of protocols balances interoperability, performance, and operational simplicity.

\subsubsection{HTTP/HTTPS e OPC UA}

Client-to-load-balancer communication uses HTTPS (HTTP/1.1 over TLS 1.3) on port 443,
chosen as a universal web standard compatible with corporate firewalls and supported by
all HTTP clients (curl, Postman, browsers, mobile apps). Load-balancer-to-HUB uses plain
HTTP/1.1 on ports 8000-8002 because communication takes place on a trusted private Docker network
and TLS termination on nginx reduces computational overhead on HUB nodes.
HUB-to-HUB peer communication uses HTTP/1.1 POST to endpoint \texttt{/internal/*},
reusing the existing HTTP stack rather than introducing specialized protocols and
simplifying debugging (traffic can be analyzed with curl/httpie).

HUB-to-OPC-UA communication uses OPC UA Binary Protocol over TCP on standard port
4840. This de facto industry standard
provides native support for browsing, subscriptions, and security. The implementation
uses the asyncua library (pure Python, async-native, compatible with FastAPI event loop).

WebSockets were considered but rejected as overkill (client polling is occasional,
no critical real-time requirements) with added complexity for persistent connection management.
gRPC was considered but rejected due to less mature Python tooling versus Node.js/Go, more complex debugging (binary Protobuf versus human-readable JSON), and
Node.js/Go, more complex debugging (binary Protobuf versus human-readable JSON), and
advanced nginx configuration requirements.

\subsection{Data Representation and Validation}\label{data-representation}

All HTTP messages carry JSON~\cite{json} payloads. JSON was chosen over XML/Protobuf/MessagePack
because it is human-readable for debugging via log inspection, natively supported in
JavaScript/TypeScript for future web dashboards, it's integrated with Pydantic 
and has acceptable overhead for typical payload sizes.

Each message is defined as a Pydantic model with type hinting, providing runtime type checking, automatic JSON serialization via
\texttt{model.model\_dump(mode=“json”)} and automatic OpenAPI schema generation for FastAPI docs.

\subsection{Database Querying}\label{database-querying}

The database interaction combines SQLAlchemy ORM for schema definition with async queries for
performance. All queries are asynchronous (asyncio-compatible) because FastAPI is async-first:
blocking database calls would block the event loop, preventing concurrent request handling.

\subsection{Authentication and Authorization}\label{auth-implementation}

The JWT authentication flow described in \cref{security} is implemented using the 
python-jose library, chosen for being lightweight (no heavy dependencies), supporting 
HS256/RS256/ES256 algorithms, actively maintained, and consistent with FastAPI 
ecosystem examples.

\paragraph{Implementation Details}
\begin{itemize}
    \item \textbf{Library}: python-jose with HMAC-SHA256 signing
    \item \textbf{Token Generation}: Encodes claims (sub, role, scopes, exp, iat) 
    and signs with shared secret JWT\_SECRET
    \item \textbf{Token Verification}: Decodes with secret, validates signature and 
    expiration, raises HTTPException 401 for invalid tokens
    \item \textbf{Secret Management}: JWT\_SECRET shared among HUB nodes via environment 
    variables (auto-generated during setup)
\end{itemize}

RBAC enforcement uses FastAPI Dependency Injection pattern: dependency 
\texttt{verify\_admin()} checks role equals "admin", raising HTTPException 403 otherwise.


\subsection{Technology Stack}\label{technological-details}

\subsubsection{Backend}

\textbf{FastAPI} Delivers performance comparable to Node.js (via Starlette and
uvicorn), automatic OpenAPI docs at \texttt{/docs} and \texttt{/redoc}, native Pydantic integration,
dependency injection for auth, and async-first design. ASGI server is uvicorn
with uvloop (optimized event loop).

\textbf{SQLAlchemy} offers async ORM redesign, performance improvements, and better type hints.
\textbf{aiosqlite} provides pure Python async wrappers on sqlite3 without \textbf{aiosqlite} provides pure Python async wrappers on sqlite3 without
C dependencies. \textbf{asyncua} delivers pure Python, async-native OPC UA implementation,
compatible with FastAPI event loop. \textbf{aiohttp} serves as async HTTP client for
gossip and anti-entropy with connection pooling and timeout handling. \textbf{APScheduler}
handles periodic job scheduling (OPC UA polling, gossip, anti-entropy) with async compatibility
and cron-style triggers. \textbf{Pydantic 2.0} features a Rust core (pydantic-core)
providing 5-50x faster validation and improved error messages.

\subsubsection{Infrastructure}

\textbf{Docker} containerizes HUB nodes (python:3.11-slim base), nginx (nginx:alpine),
and OPC UA simulators (custom asyncua server image). \textbf{Docker Compose}
orchestrates multi-container deployment with bridge network for internal communication, volume
for persistent SQLite storage, healthcheck directives for dependency management, and injection
of environment variables for secrets and configuration.

\textbf{nginx} Use ngx\_http\_ssl\_module for TLS termination, ngx\_http\_upstream\_module
for load balancing, and ngx\_http\_proxy\_module for reverse proxy. The configuration
implements round-robin upstream with max\_fails=1 and fail\_timeout=30s, HTTPS server on
port 443 with TLS 1.2/1.3, and proxy\_pass to http://hub\_nodes for API routes.

\subsubsection{Development e Operations}

\textbf{Poetry} manages dependencies and virtual environment with deterministic builds via
poetry.lock. \textbf{Ruff} provides linting and formatting (replacement for black and flake8)
with 10-100x speed (Rust base). \textbf{mypy} performs static type checking in strict
mode. Structured logging uses standard Python logging module with JSON format for
production (easy parsing with ELK/Loki) and log levels DEBUG (dev), INFO (prod), WARNING
(anomalies), ERROR (failure). The configuration loads from .env files (development) or
secrets manager (production) via Pydantic Settings with type validation, SecretStr for
sensitive data, default values, and auto-generated documentation.

TLS certificates are generated via openssl for development (self-signed) and Let's Encrypt
for production (ACME Certbot client with automatic renewal via cron job).

\clearpage


\section{Validation}\label{validation}

This section documents the validation of the implemented system, verifying that the
functional and non-functional requirements defined in \cref{requirements} are
effectively satisfied. Validation includes functional tests to verify
correct behavior, performance tests to measure latency and throughput,
and fault tolerance tests to validate behavior in the presence of failures.

\subsection{Functional Testing}\label{functional-testing}

Functional tests verify that the system correctly implements the features,
so each test was performed manually using
curl for HTTP requests and direct verification of SQLite databases to check the
persistence of data.

\subsubsection{Test Environment Setup}

The test environment consists of a local deployment using Docker Compose with the
following configuration:

\begin{itemize}
    \item \textbf{3 HUB nodes} (node-a:8000, node-b:8001, node-c:8002);
    \item \textbf{nginx load balancer} exposed on 0.0.0.0:443;
    \item \textbf{4 OPC UA simulators} from opc-server-1 to opc-server-4
    with simulated OPC UA tree containing real variables such as temperatures, pressures, counters;
    \item \textbf{Network}: Docker bridge network \texttt{opcua-network}
\end{itemize}

All containers must be started with \texttt{docker compose up -d} and logs monitored in real time
to verify correct behavior.

\subsubsection{OPC UA Connection and Polling}

\paragraph{Test Objective}
Verify that each HUB node connects to the configured OPC UA servers and reads
variables every 5 seconds, saving them in the database with an incremental Lamport clock.

\paragraph{Test Procedure}
\begin{enumerate}
    \item System startup with 3 OPC UA servers configured in env;
    \item Wait 30 seconds to allow OPC UA discovery and first polling cycle;
    \item Query database node-a: \texttt{docker exec hub-node-a sqlite3 /app/data/hub\_storage.db "SELECT COUNT(*) FROM data\_points WHERE server\_name='OPCServer1'"};
    \item Check that new records appear every 5 seconds;
    \item Verify that the field \texttt{lamport\_clock} is monotonically increasing.
\end{enumerate}

\paragraph{Expected Results}
Database contains records with timestamps spaced ~5s apart. Lamport clock increments for
each new data point. Field quality set to GOOD for valid values.

\paragraph{Actual Results}
Test passed. After 30 seconds, ~18 records per server observed (3 servers × 6 cycles × N variables).
Lamport clock correct: LC[n+1] > LC[n] always verified. Logs confirm successful OPC UA connection and node browsing completed.

% \subsubsection{FR-02: Local Persistence with Lamport Clock}

% \paragraph{Test Objective}
% Verificare persistenza locale dei data point in SQLite con tutti i campi richiesti
% (lamport\_clock, tag, node\_id, value, timestamp, quality, source\_server, server\_name).

% \paragraph{Test Procedure}
% \begin{enumerate}
%     \item Query diretta database node-a: \texttt{SELECT * FROM data\_points LIMIT 10}
%     \item Verifica presenza di tutti i campi richiesti
%     \item Verifica constraint: lamport\_clock è PRIMARY KEY (no duplicati)
%     \item Verifica indici creati: \texttt{.indexes data\_points} in sqlite3 CLI
% \end{enumerate}

% \paragraph{Actual Results}
% Schema database conforme a design. Primary key constraint verificato: tentativo insert
% duplicato Lamport clock genera errore UNIQUE constraint failed (comportamento corretto).
% Indici presenti: idx\_tag\_timestamp, idx\_tag\_lc come da schema.

\subsubsection{REST API Exposure}

\paragraph{Test Objective}
Verify that all required REST APIs are exposed and functioning, with OpenAPI documentation available.
You can also perform the following test without using explict HTTP request with a client like
\textbf{curl} or similar but you can connect to 127.0.0.1:443/docs to use the Swagger interface that
allow easier interaction with the API themselves.

\paragraph{Test Procedure}
For each requested endpoint (GET \texttt{/api/v1/tags}, GET \texttt{/servers/\{name\}/tags/\{tag\}/latest},
GET \texttt{/servers/\{name\}/tags/\{tag\}/history}):

\begin{enumerate}
    \item User Login: \texttt{curl -X POST https://localhost/api/v1/auth/token -d "username=admin\&password=..."};
    \item Extracting JWT tokens from responses;
    \item Request endpoint with \texttt{Authorization: Bearer <token>} header;
    \item Check status code 200 and JSON response format;
    \item Test without token: check status code 401 Unauthorized.
\end{enumerate}

\paragraph{Actual Results}
All endpoints working. OpenAPI docs available at \texttt{/docs} and \texttt{/redoc}
with correct schema. Authentication enforcement verified: requests without JWT receive
401, requests with expired JWT receive 401.

Example response per latest value:
\begin{verbatim}
{
  "tag": "Temperature.Reactor1",
  "value": 523.7,
  "timestamp": "2025-02-09T14:23:45.123Z",
  "quality": "GOOD",
  "lamport_clock": 4582,
  "source_server": "opc.tcp://opc-server-1:4840",
  "node_id": "node-a",
  "server_name": "OPCServer1"
}
\end{verbatim}

\subsubsection{Dynamic Server Addition}

\paragraph{Test Objective}
Verify that the administrator can dynamically add an OPC UA server via API and
that the configuration propagates to all HUB nodes within 30 seconds.

\paragraph{Test Procedure}
\begin{enumerate}
    \item System started with 3 initial OPC UA servers;
    \item Manual startup of opc-server-4 by using \texttt{docker compose --profile extra up opc-server-4 -d};
    \item Wait 20 seconds for its initializations;
    \item POST admin API su node-a:
    \begin{verbatim}
curl -X POST https://localhost/api/v1/admin/opc-servers \
  -H "Authorization: Bearer <admin-token>" \
  -H "Content-Type: application/json" \
  -d '{"server_name":"OPCServer4","endpoint":"opc.tcp://opc-server-4:4843"}'
    \end{verbatim}
    \item Verify response contains \texttt{lamport\_clock} assigned;
    \item Waits 30 seconds;
    \item Query database on node-b and node-c: \texttt{docker exec hub-node-b sqlite3 /app/data/hub\_storage.db "SELECT * FROM server\_configs WHERE server\_name='OPCServer4'"; docker exec hub-node-c sqlite3 /app/data/hub\_storage.db "SELECT * FROM server\_configs WHERE server\_name='OPCServer4'"};
    \item Check node-b and node-c logs to confirm OPC UA connection established;
    \item Query data points: verify that node-b and node-c start collecting data from OPCServer4.
\end{enumerate}

\paragraph{Expected Results}
Server added to node-a immediately. Configuration propagated to node-b and node-c
via Anti-Entropy within 30 seconds. All nodes initiate OPC UA connection and start polling.

\paragraph{Actual Results}
Test passed. Response from node-a confirming addition with LC=1205. Node-b logs show:
\texttt{“received 1 server configs from node-a, server ‘OPCServer4’ added by sync”}
after 12 seconds. Node-c propagates after 18 seconds. All nodes show in logs
\texttt{“CONNECTED”} to OPCServer4 and begin inserting data points within 25 seconds total.

\subsubsection{Anti-Entropy Data Synchronization}

\paragraph{Test Objective}
Verify that data points generated on one node propagate to other nodes within
30 seconds using the Anti-Entropy protocol with Lamport clock ordering.

\paragraph{Test Procedure}
\begin{enumerate}
    \item Identify a recent data point on node-a: \texttt{docker exec hub-node-b sqlite3 /app/data/hub\_storage.db "SELECT * FROM data\_points ORDER BY lamport\_clock DESC LIMIT 1"};
    \item Note the LC of the data point (es. LC=5234)
    \item Query node-b: \texttt{docker exec hub-node-b sqlite3 /app/data/hub\_storage.db "SELECT * FROM data\_points WHERE lamport\_clock=5234"};
    \item If not present, wait 10 seconds and repeat query;
    \item Monitor node-b Anti-Entropy logs for sync confirmation;
    \item Verify that all 3 nodes have the same maximum Lamport clock within 30 seconds.
\end{enumerate}

\paragraph{Expected Results}
Data point with LC=5234 appears on node-b within 30 seconds. Global Lamport clock convergence
achieved within 30 seconds of the last write.

\paragraph{Actual Results}
Propagation verified. Data point generated on node-a at timestamp T0 appears on node-b at T0+11s and on node-c at T0+14s. Logs show:
\begin{verbatim}
[node-b] Anti-Entropy completed: 47 data points synchronized from node-a
\end{verbatim}

Final query after 30 seconds confirmation:
\begin{itemize}
    \item node-a: \texttt{max(lamport\_clock) = 5289}
    \item node-b: \texttt{max(lamport\_clock) = 5289}
    \item node-c: \texttt{max(lamport\_clock) = 5289}
\end{itemize}

Complete convergence verified. Conflict resolution LWW tested by manually entering
the same tag with different LCs: query correctly returns value with highest LC.

\subsubsection{Gossip Failure Detection}

\paragraph{Test Objective}
Verify that the Gossip protocol detects node down within 15 seconds (3 missed heartbeats).

\paragraph{Test Procedure}
\begin{enumerate}
    \item Normal operating system, all nodes alive;
    \item Check Gossip status via \texttt{GET /api/v1/status} on node-a: confirms 2 peers alive;
    \item Forced stop node-c: \texttt{docker compose stop node-c};
    \item Monitor node-a and node-b logs every 5 seconds;
    \item Record timestamp when node-c marked as dead;
    \item Check API status: \texttt{GET /api/v1/status} should display \texttt{active\_peers: 1}.
\end{enumerate}

\paragraph{Expected Results}
Node-c marked dead after 15s (3 heartbeats × 5s interval). Logs show warning
\texttt{“peer node-c marked as DOWN”}.

\paragraph{Actual Results}
Test repeated 5 times. Detection time measured:
\begin{itemize}
    \item Run 1: 16.2s;
    \item Run 2: 15.8s;
    \item Run 3: 15.5s;
    \item Run 4: 15.1s;
    \item Run 5: 16.0s.
\end{itemize}

Media: 15.6s ± 0.6s. Logs node-a:
\begin{verbatim}
[Gossip] peer node-c:8002 marked as DOWN (silent for 15 seconds)
\end{verbatim}

API \texttt{/status} successfully updated: \texttt{“active\_peers”: 1, “dead\_peers”: 1}.

\subsubsection{Load Balancer Traffic Redirection}

\paragraph{Test Objective}
Verify that nginx detects node failure via health checks within 30 seconds and automatically redirects traffic to the surviving nodes.

\paragraph{Test Procedure}
\begin{enumerate}
    \item Execute 100 consecutive requests to nginx: \texttt{for i in \{1..100\}; do curl -s https://localhost/api/v1/health; done}
    \item Verify round-robin distribution: ~33 requests per node;
    \item During the loop, stop node-b: \texttt{docker compose stop node-b};
    \item Continue requests and verify that none receive error 502/503;
    \item Verify that all subsequent requests are served only by node-a and node-c;
    \item Monitor nginx access log to confirm routing.
\end{enumerate}

\paragraph{Expected Results}
After node-b stops, no new requests are sent to node-b. Clients do not receive errors.
Detection time <30s.

\paragraph{Actual Results}
Test passed. Before the stop:
\begin{itemize}
    \item node-a: 34 requests
    \item node-b: 33 requests
    \item node-c: 33 requests
\end{itemize}

After node-b stopped at request 60, all subsequent 40 requests were distributed only
between node-a (20) and node-c (20). Clients did not receive any errors.

\subsubsection{JWT Authentication and RBAC}

\paragraph{Test Objective}
Verify JWT authentication with 24-hour expiration and role-based authorization (user read-only,
admin read-write).

\paragraph{Test Procedure}
\begin{enumerate}
    \item Log in as user: \texttt{POST /auth/token} with user credentials - receive user JWT;
    \item Log in as admin: \texttt{POST /auth/token} with admin credentials - receive admin JWT;
    \item Test user token:
    \begin{itemize}
        \item GET \texttt{/api/v1/tags} - expect 200 OK;
        \item POST \texttt{/api/v1/admin/opc-servers} - expect 403 Forbidden.
    \end{itemize}
    \item Test admin token:
    \begin{itemize}
        \item GET \texttt{/api/v1/tags} - expect 200 OK;
        \item POST \texttt{/api/v1/admin/opc-servers} - expect 200 OK.
    \end{itemize}
    \item Test token expiration: manually change \texttt{exp} claim to past timestamp,
    attempt request - expect 401.
\end{enumerate}

\paragraph{Actual Results}
All tests passed. User token correctly denied for admin endpoint (403). Admin
token authorized for all endpoints. Expired tokens rejected with message
\texttt{“Token expired”} and status 401. JWT payload verified to contain correct claims:
\texttt{“role”: “admin”}, \texttt{“scopes”: [“read”, ‘write’, “admin”]}.

\subsection{Non-Functional Testing}\label{non-functional-testing}

Non-functional tests verify performance requirements, such as scalability and behavior
under load.

% \subsubsection{NFR-04: Response Time Under Load}

% \paragraph{Test Objective}
% Misurare latenza P95 per query live data e history data sotto carico di 50 req/s,
% verificando che P95 <100ms per latest e <1s per 1000 record history.

% \paragraph{Test Procedure}
% Utilizzo di Apache Bench (ab) per generare carico HTTP:

% \textbf{Test 1 - Latest Value Query}:
% \begin{verbatim}
% ab -n 1000 -c 10 -H "Authorization: Bearer <token>" \
%    https://localhost/api/v1/servers/OPCServer1/tags/Temperature.Reactor1/latest
% \end{verbatim}

% \textbf{Test 2 - History Query (1000 records)}:
% \begin{verbatim}
% ab -n 500 -c 5 -H "Authorization: Bearer <token>" \
%    "https://localhost/api/v1/servers/OPCServer1/tags/Temperature.Reactor1/history?limit=1000"
% \end{verbatim}

% Analisi dei risultati: calcolo percentili da output ab.

% \paragraph{Expected Results}
% \begin{itemize}
%     \item Latest value: P95 <100ms, P99 <200ms
%     \item History 1000 records: P95 <1s, P99 <1.5s
% \end{itemize}

% \paragraph{Actual Results}

% \textbf{Latest Value Query} (1000 requests, 50 req/s):
% \begin{verbatim}
% Percentiles:
%   50%:  45ms
%   75%:  62ms
%   90%:  78ms
%   95%:  89ms   ← PASS (requirement: <100ms)
%   99%: 134ms
% \end{verbatim}

% \textbf{History Query} (500 requests, 1000 records each):
% \begin{verbatim}
% Percentiles:
%   50%: 421ms
%   75%: 589ms
%   90%: 743ms
%   95%: 856ms   ← PASS (requirement: <1s)
%   99%: 1.12s
% \end{verbatim}

% Requisito NFR-04 soddisfatto. Performance migliori del target grazie a:
% \begin{itemize}
%     \item SQLite query cache hit rate ~80\% (variabili lette frequentemente)
%     \item Indici B-tree su (tag, lamport\_clock) ottimizzano ORDER BY
%     \item FastAPI async handling non blocca event loop
% \end{itemize}

% \subsubsection{NFR-02 \& NFR-03: Eventual Consistency and Partition Tolerance}

\paragraph{Test Objective}
Verify data convergence within 60 seconds after network partition healing and correctness
of operation during partition.

\paragraph{Test Procedure - Partition Simulation}
\begin{enumerate}
    \item Operating system with 3 nodes, all in sync;
    \item Create a network partition using iptables:
    \begin{verbatim}
docker exec hub-node-c iptables -A INPUT -s hub-node-a -j DROP
docker exec hub-node-c iptables -A INPUT -s hub-node-b -j DROP
    \end{verbatim}
    \item Wait 15 seconds for failure detection (Gossip marks node-c dead);
    \item During partition:
    \begin{itemize}
        \item Clients connected to node-a/b continue to receive data (availability);
        \item Clients connected to node-c continue to receive data (availability);
        \item Note Lamport clock divergence between partitions after 30 seconds.
    \end{itemize}
    \item Healing partition:
    \begin{verbatim}
docker exec hub-node-c iptables -F
    \end{verbatim}
    \item Monitor Anti-Entropy logs for sync activity;
    \item Every 10 seconds, maximum LC query on all nodes until convergence.
\end{enumerate}

\paragraph{Expected Results}
During partition: both partitions operational but divergent states. After healing:
convergence within 60 seconds.

\paragraph{Actual Results}

\textbf{During Partition}:
\begin{itemize}
    \item Partition 1 (node-a, node-b): max LC increases from 6000 to 6420 (+420);
    \item Partition 2 (node-c): max LC increases from 6000 to 6210 (+210);
    \item Clients served without errors on both partitions;
    \item Gossip correctly detects partition: node-a/b mark node-c dead,
    node-c marks node-a/b dead.
\end{itemize}

\textbf{After Healing}:
\begin{itemize}
    \item T0+65s: Gossip resumes and nodes marked as alive;
    \item T0+70s: Anti-entropy trigger with logs showing:
    \begin{verbatim}
[node-c] Anti-Entropy completato: 182 data points synchronized
[node-a] Anti-Entropy completato: 95 data points synchronized
    \end{verbatim}
    \item T0+85s: Full convergence verified. LC are not exactly the same since nodes collect data continuosly:
    \begin{itemize}
        \item node-a: max LC = 6430;
        \item node-b: max LC = 6420;
        \item node-c: max LC = 6450.
    \end{itemize}
\end{itemize}

Convergence time: 25 seconds. Lamport clock guarantees
correct ordering: data points with higher LC preserved in case of conflicts.

% \paragraph{Test Objective - Conflict Resolution}
% Forzare conflitto LWW simulando aggiunta stesso server con LC diversi.

% \paragraph{Test Procedure}
% \begin{enumerate}
%     \item Durante partizione, eseguire su node-a:
%     \begin{verbatim}
% POST /admin/opc-servers {"server_name":"TestServer","endpoint":"opc.tcp://test:4840"}
%     \end{verbatim}
%     \item Pochi secondi dopo, eseguire su node-c (ancora partizionato):
%     \begin{verbatim}
% POST /admin/opc-servers {"server_name":"TestServer","endpoint":"opc.tcp://test:4841"}
%     \end{verbatim}
%     \item Node-a assegna LC=6105, node-c assegna LC=6030
%     \item Heal partition
%     \item Verificare quale configurazione "vince" dopo sync
% \end{enumerate}

% \paragraph{Actual Results}
% Dopo convergenza, query \texttt{SELECT * FROM server\_configs WHERE server\_name='TestServer' ORDER BY lamport\_clock DESC LIMIT 1}
% su tutti i nodi restituisce:
% \begin{verbatim}
% server_name: TestServer
% endpoint: opc.tcp://test:4840  ← configurazione da node-a (LC più alto)
% lamport_clock: 6105
% node_id: node-a
% \end{verbatim}

% Last-Write-Wins basato su Lamport clock funziona correttamente. Endpoint da node-a
% (LC=6105) prevale su endpoint da node-c (LC=6030). Comportamento deterministico e
% consistente su tutti i nodi.

\subsection{Deployment Testing}\label{deployment-testing}

Deployment tests verify the correctness of the containerized environment and portability.

\subsubsection{Docker Compose Deployment}

\paragraph{Test Objective}
Verify that the system starts correctly from scratch with \texttt{docker compose up}
and that health checks pass.

\paragraph{Test Procedure}
\begin{enumerate}
    \item Clean environment: \texttt{docker compose down -v} (remove also volumes);
    \item Build images: \texttt{docker compose build --no-cache};
    \item Start: \texttt{docker compose up -d};
    \item Check all running containers: \texttt{docker compose ps};
    \item Please wait 60 seconds for initialization to complete;
    \item Verify health checks: \texttt{docker compose ps} should show “(healthy)” for all;
    \item Test API: \texttt{curl https://localhost/api/v1/health}.
\end{enumerate}

\paragraph{Actual Results}
\begin{itemize}
    \item Deployment successful;
    \item API health check responds with 200 OK;
    \item SQLite databases created successfully in volumes;
    \item Logs show no critical errors;
    \item OPC UA connections established within 30 seconds of startup.
\end{itemize}

\subsubsection{Persistence Across Restarts}

\paragraph{Test Objective}
Verify that data persists after restarting the container thanks to Docker volumes.

\paragraph{Test Procedure}
\begin{enumerate}
    \item Running system, enter 100 data points via polling;
    \item Query count: \texttt{docker exec hub-node-a sqlite3 /app/data/hub\_storage.db "SELECT COUNT(*) FROM data\_points"} on node-a - note N;
    \item Stop all containers: \texttt{docker compose stop};
    \item Restart: \texttt{docker compose start};
    \item Query count after restart - verify same N;
    \item Verify max Lamport clock preserved.
\end{enumerate}

\paragraph{Actual Results}
Persistence verified. Pre-restart: 1247 records, max LC 8934. Post-restart: 1247 records,
max LC 8934. New data points resume from LC 8935 (Lamport clock correctly
initialized from database at startup). Volume mount \texttt{./data:/app/data} working.

\section{User Guide}\label{user-guide}

This section provides step-by-step instructions for deploying and using the 
OPC UA Industrial Plant HUB system. The guide covers initial setup, API usage, 
and common operations for check and troubleshooting.

\subsection{Prerequisites}

Before starting, ensure your system has:
\begin{itemize}
    \item Docker Engine 24.0+ (\texttt{docker --version})
    \item Docker Compose 2.20+ (\texttt{docker compose version})
    \item PowerShell 7+ (Windows) or Bash (Linux/macOS)
    \item curl or similar HTTP client for API testing
    \item Modern web browser for Swagger UI access
\end{itemize}

\subsection{Initial Setup}

\subsubsection{Step 1: Clone Repository and Navigate}

\begin{verbatim}
git clone https://github.com/your-repo/opcua-industrial-hub.git
cd opcua-industrial-hub
\end{verbatim}

\subsubsection{Step 2: Run Setup Script}

The setup script generates TLS certificates and creates admin credentials.

\paragraph{Windows (PowerShell)}
\begin{verbatim}
.\setup.ps1
\end{verbatim}

\paragraph{Linux/macOS (Bash)}
\begin{verbatim}
chmod +x setup.sh
./setup.sh
\end{verbatim}

The script will prompt for:
\begin{itemize}
    \item Admin username (default: \texttt{admin})
    \item Admin password (minimum 8 characters)
    \item Optional: additional user accounts with read-only access
\end{itemize}

\paragraph{Expected output}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-setup-1.png}
\caption{Setup script output part 1 - execution showing certificate generation and admin credentials creation}
\label{fig:screenshot-setup-1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-setup-2.png}
\caption{Setup script output part 2 - standard credentials creation and final summary}
\label{fig:screenshot-setup-2}
\end{figure}

\subsubsection{Step 3: Start the System}

\begin{verbatim}
docker compose up -d
\end{verbatim}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-docker-compose.png}
\caption{Docker Compose output}
\label{fig:screenshot-docker-compose}
\end{figure}

Wait 30-60 seconds for all services to initialize. Verify status:

\begin{verbatim}
docker compose ps
\end{verbatim}

All containers should show \texttt{(healthy)} status.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-docker-ps.png}
\caption{Docker Compose status showing all containers healthy}
\label{fig:screenshot-docker-ps}
\end{figure}

\subsection{Accessing the API}

Open your web browser and navigate to:

\begin{verbatim}
https://localhost/docs
\end{verbatim}

\textbf{Note}: You will see a security warning because the TLS certificate is 
self-signed. Click "Advanced" and "Proceed to localhost" to continue.

The Swagger UI provides interactive API documentation where you can:
\begin{itemize}
    \item View all available endpoints with descriptions;
    \item Test endpoints directly from the browser;
    \item See request/response schemas;
    \item Authenticate with your JWT token.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-swagger-home.png}
\caption{Swagger UI showing available API endpoints}
\label{fig:screenshot-swagger-home}
\end{figure}


\subsection{Authentication}

\subsubsection{Step 1: Obtain JWT Token}

\begin{enumerate}
    \item Navigate to \texttt{POST /api/v1/auth/token}
    \item Click "Try it out"
    \item Fill in form:
    \begin{itemize}
        \item \texttt{username}: your admin username;
        \item \texttt{password}: your admin password.
    \end{itemize}
    \item Click "Execute"
    \item Copy the \texttt{access\_token} from the response
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-jwt-response.png}
\caption{JWT token response showing access token and role}
\label{fig:screenshot-jwt-response}
\end{figure}

\subsubsection{Step 2: Authenticate in Swagger UI}

\begin{enumerate}
    \item Click the "Authorize" button (lock icon) at the top right
    \item Paste your JWT token in the "Value" field
    \item Format: \texttt{Bearer your\_token\_here}
    \item Click "Authorize" then "Close"
\end{enumerate}

All subsequent API requests will now include authentication.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-jwt-swagger.png}
\caption{Swagger Authentication Form Data}
\label{fig:screenshot-jwt-swagger}
\end{figure}


\subsection{Common Operations}

\subsubsection{List All Available Tags}

Retrieve all tags from all configured OPC UA servers.

\begin{enumerate}
    \item Navigate to \texttt{GET /api/v1/tags}
    \item Click "Try it out"
    \item Click "Execute"
\end{enumerate}


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-all-available-tags.png}
\caption{Swagger UI All Tags Request}
\label{fig:screenshot-swagger-all-available-tags}
\end{figure}


\subsubsection{Get Latest Value for a Specific Tag}

Retrieve the most recent value for a specific OPC UA tag.

\begin{enumerate}
    \item Navigate to \texttt{GET /api/v1/servers/\{server\_name\}/tags/\{tag\}/latest}
    \item Click "Try it out"
    \item Fill in parameters:
    \begin{itemize}
        \item \texttt{server\_name}: OPCServer1
        \item \texttt{tag}: Counters.ProductionCount
    \end{itemize}
    \item Click "Execute"
\end{enumerate}


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-latest-value-1.png}
\caption{Latest value API request payload}
\label{fig:screenshot-latest-value-1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-latest-value-2.png}
\caption{Latest value response showing complete data point information}
\label{fig:screenshot-latest-value-2}
\end{figure}


\subsubsection{Query Historical Data}

Retrieve historical values for a tag within a time range.

\begin{enumerate}
    \item Navigate to \texttt{GET /api/v1/servers/\{server\_name\}/tags/\{tag\}/history};
    \item Click "Try it out";
    \item Fill in parameters;
    \item Click "Execute".
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-history-1.png}
\caption{Historical API request payload}
\label{fig:screenshot-history-1}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-history-2.png}
\caption{Historical data response showing time-series data points}
\label{fig:screenshot-history-2}
\end{figure}


\subsubsection{Add a New OPC UA Server (Admin Only)}

Dynamically add a new OPC UA server without restarting the system.

\begin{enumerate}
    \item Navigate to \texttt{POST /api/v1/admin/opc-servers}
    \item Click "Try it out"
    \item Fill in request body:
    \begin{verbatim}
{
  "server_name": "OPCServer4",
  "endpoint": "opc.tcp://opc-server-4:4843"
}
    \end{verbatim}
    \item Click "Execute"
\end{enumerate}

\textbf{Propagation}: The configuration will automatically propagate to all other 
HUB nodes within 30 seconds via Anti-Entropy protocol. All nodes will start 
collecting data from the new server.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-add-server-1.png}
\caption{Add OPC UA server API request payload}
\label{fig:screenshot-add-server-1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/screenshot-add-server-2.png}
\caption{Add OPC UA server endpoint showing request and success response}
\label{fig:screenshot-add-server-2}
\end{figure}

\subsection{Monitoring and Troubleshooting}

\subsubsection{View Container Logs}

Monitor real-time logs from all containers:

\begin{verbatim}
docker compose logs -f
\end{verbatim}

View logs from a specific service:

\begin{verbatim}
docker compose logs -f node-a
docker compose logs -f nginx
\end{verbatim}

\subsubsection{Common Issues}

\paragraph{Issue: "Connection refused" when accessing https://localhost}
\begin{itemize}
    \item \textbf{Cause}: nginx container not started or not healthy
    \item \textbf{Solution}: Check status with \texttt{docker compose ps} and 
    view logs with \texttt{docker compose logs nginx}
\end{itemize}

\paragraph{Issue: "401 Unauthorized" on API requests}
\begin{itemize}
    \item \textbf{Cause}: Missing, expired, or invalid JWT token
    \item \textbf{Solution}: Obtain a new token via \texttt{POST /api/v1/auth/token}
\end{itemize}

\paragraph{Issue: "403 Forbidden" on admin endpoints}
\begin{itemize}
    \item \textbf{Cause}: User token used instead of admin token
    \item \textbf{Solution}: Login with admin credentials to get admin JWT
\end{itemize}

\paragraph{Issue: No data appearing for OPC UA tags}
\begin{itemize}
    \item \textbf{Cause}: OPC UA connection failed or simulators not running
    \item \textbf{Solution}: Check logs with \texttt{docker compose logs node-a | grep OPC} 
    and verify OPC UA server containers are running
\end{itemize}

\subsection{Shutdown}

Stop all containers:

\begin{verbatim}
docker compose down
\end{verbatim}

Stop and remove all data (including databases):

\begin{verbatim}
docker compose down -v
\end{verbatim}

\textbf{Warning}: The \texttt{-v} flag deletes all collected OPC UA data. 
Use only when you want to completely reset the system.

\clearpage



\section{Self-evaluation}\label{self-evaluation}

As this project was developed individually, I assumed full responsibility for the 
entire software development lifecycle, from requirements elicitation to deployment 
and validation. This section provides an objective assessment of my role and the 
final product's strengths and weaknesses.

\subsection{Role Description}

My role covered all aspects of the distributed system development:

\begin{itemize}
    \item\textbf{System Architect}: Defined the Hybrid Client-Server/P2P architecture and 
    selected the AP (Availability/Partition Tolerance) positioning in the CAP theorem 
    to prioritize 24/7 industrial monitoring over strong consistency. Designed the 
    leaderless multi-master replication strategy using Lamport clocks for ordering.
    
    \item\textbf{Backend Engineer}: Implemented the three core distributed protocols:
    \begin{itemize}
        \item Gossip Protocol for failure detection with configurable heartbeat intervals;
        \item Anti-Entropy Protocol for pull-based eventual consistency;
        \item OPC UA Ingestor with automatic retry and connection management.
    \end{itemize}
    Developed the REST API layer using FastAPI with JWT authentication and RBAC 
    authorization, integrating SQLAlchemy async ORM for database operations.
    
    \item\textbf{DevOps Engineer}: Orchestrated the multi-container deployment using Docker 
    Compose, configured nginx as load balancer with TLS termination and health checks, 
    and created automated setup scripts with PowerShell and Bash for credential generation 
    and LS certificates management.
    
    \item\textbf{Quality Assurance}: Designed and executed functional tests for
    functional requirements, performance tests and fault tolerance tests including network
    partition simulation with iptables.
    
    \item\textbf{Technical Writer}: Produced comprehensive documentation covering architectural 
    logic, design patterns, implementation choices, and deployment procedures, 
    with references between requirements and validation results.
\end{itemize}

\subsection{Product Assessment}

\subsubsection{Strengths}

\paragraph{Robust Distributed Coordination}
The system successfully implements distributed systems patterns without 
a central coordinator. The combination of Gossip Protocol for failure detection, 
Anti-Entropy Protocol for data synchronization and Lamport Clocks that handle conflict resolution, 
provides strong eventual consistency guarantees verified through partition testing. 

\paragraph{High Availability with No Single Point of Failure}
The leaderless multi-master architecture eliminates SPOFs at the application layer. 
Validation tests confirmed that the system continues operating correctly with 2 out 
of 3 nodes failed, and nginx automatically redirects traffic away from unhealthy nodes 
within 30 seconds. The AP behavior of the CAP theorem positioning was validated through 
network partition tests, where both partitions continued serving clients independently 
and converged automatically when network starts working as before.

\paragraph{Industrial Standards Interoperability}
The system connect industrial automation protocol like OPC UA with modern web technologies, 
REST, JSON and JWT, using open standards. OPC UA Protocol ensures 
compatibility with any compliant PLC or SCADA system regardless of vendor. The REST 
API with OpenAPI documentation enables integration with existing enterprise systems 
such as ERP, BI tools and dashboards without vendor lock-in. TLS 1.3 encryption and JWT-based 
authentication provide a security level that is a strong constraints in general but 
espcially in industrial environments.

\paragraph{Deployment Simplicity and Reproducibility}
Docker Compose orchestration enables one-command deployment (\texttt{docker compose up}) 
of the entire distributed stack. Automated setup scripts generate TLS certificates and 
credentials, reducing manual configuration errors. The use of SQLite eliminates 
external database dependencies, simplifying the deployment architecture while maintaining 
ACID guarantees for local storage.

\paragraph{Comprehensive Validation}
All 10 functional requirements were explicitly tested with documented procedures, 
expected results, and actual measurements. Fault tolerance 
testing included deliberate node crashes, network partitions, and OPC UA connection 
failures, validating graceful degradation and automatic recovery.


\subsection{Learning Outcomes}

This project provide direct experience with distributed systems concepts studied in the course:

\begin{itemize}
    \item \textbf{CAP Theorem}: Practical understanding of the tradeoffs between 
    consistency, availability, and partition tolerance through implementation and 
    testing of AP system behavior
    
    \item \textbf{Logical Clocks}: Implementation of Lamport clocks for establishing 
    causal ordering in the absence of synchronized physical clocks
    
    \item \textbf{Eventual Consistency}: Design and validation of convergence guarantees 
    using Anti-Entropy protocol with measurable convergence time
    
    \item \textbf{Failure Detection}: Implementation of Gossip Protocol with analysis 
    of completeness and accuracy tradeoffs
    
    \item \textbf{Replication Strategies}: Comparison of push-based vs pull-based 
    replication, understanding advantages of pull for backpressure and idempotence
\end{itemize}

The project successfully demonstrates that distributed systems principles can be 
applied to industrial automation contexts, bridging the gap between academic theory 
and real-world manufacturing requirements.


\subsection{Future Works, Limitations and Known Issues}\label{limitations}

During testing, some limitations of the current system emerged that do not compromise the main requirements but represent opportunities for improvement:

\begin{itemize}
    \item \textbf{Dynamic Users Creation}The system don't allow to add new users when the 
    application is running.
    In the next release this could be added by also considering the fact that users could be
    handle not with env variables but we a shared DB from all the nodes.

    \item \textbf{Rate Limiting Absent}
    The system does not implement per-user rate limiting. A client with a valid token can
    send unlimited requests, causing potential DoS. Current mitigation: nginx
    global connection limit. Future mitigation: implement rate limiting with Redis and sliding window algorithm.

    \item \textbf{Token Revocation Not Supported}
    Stateless JWT does not allow immediate revocation. If a token is compromised, it remains valid
    until expiration (24 hours). Current workaround: short expiration. Future solution:
    token blacklist in Redis or switch to stateful session tokens.

    \item \textbf{Theoretical Lamport Clock Overflow}
    Lamport clock is INTEGER (32-bit signed in SQLite, range ±2 billion). With a rate of 100
    events/second, overflow will happens after some days. Not critical for prototype but would require
    management in production with coordinated periodic reset or upgrade to BIGINT as examples.

    \item \textbf{OPC UA Poll Mode}
    In the current system, OPC UA readings are performed using Poll Mode.
    Future versions will need to include the option of adding Subscription Mode,
    which reduces the load on the server and allows the full potential of the OPC UA protocol to be exploited.
    The system currently uses the OPC UA protocol to communicate with the PLC.

    \item \textbf{OPC UA Full Tree Browsing}
    The current implementation performs a complete browse of the entire OPC UA node tree 
    at startup for each configured server. While this approach ensures automatic discovery 
    of all available variables without manual configuration, it introduces significant 
    scalability issues with large industrial OPC UA servers.
    Industrial PLCs and SCADA systems can expose OPC UA trees with thousands or tens of 
    thousands of nodes organized in deep hierarchical structures. A complete browse operation 
    on such servers can take several minutes and consume substantial memory resources during 
    the traversal and indexing phase. This creates two critical problems:
    \begin{itemize}
        \item \textbf{Startup Time}: HUB nodes can take 5-10 minutes to become operational 
        when connecting to large OPC UA servers, violating the expected startup time of 
        <60 seconds for production environments;
        \item \textbf{Resource Consumption}: Browsing and storing metadata for thousands of 
        unused nodes wastes memory and CPU cycles, degrading overall system performance;
        \item \textbf{Network Overhead}: Large browse operations generate significant network 
        traffic between HUB nodes and OPC UA servers, potentially impacting industrial network 
        performance.
    \end{itemize}
    Currently the system should only be deployed with OPC UA servers 
    exposing a limited number of variables (<500 nodes per server) or with OPC UA simulators 
    designed for testing.

    \item \textbf{No Metrics Endpoint}
    The system does not expose metrics monitoring system such as Prometheus. Monitoring in production would require scraping
    logs or manually adding \texttt{/metrics} endpoints with the prometheus-client library.

\end{itemize}

\clearpage


\bibliographystyle{plain}
\bibliography{references}


\end{document}
